{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to MLflow and OpenAI's Whisper\n",
    "\n",
    "Discover the integration of [OpenAI's Whisper](https://huggingface.co/openai), an [ASR system](https://en.wikipedia.org/wiki/Speech_recognition), with MLflow in this tutorial.\n",
    "\n",
    "### What You Will Learn in This Tutorial\n",
    "\n",
    "- Establish an audio transcription **pipeline** using the Whisper model.\n",
    "- **Log** and manage Whisper models with MLflow.\n",
    "- Infer and understand Whisper model **signatures**.\n",
    "- **Load** and interact with Whisper models stored in MLflow.\n",
    "- Utilize MLflow's **pyfunc** for Whisper model serving and transcription tasks.\n",
    "\n",
    "<details>\n",
    "    <summary style=\"cursor: pointer; display: flex; align-items: center;\">\n",
    "        <span style=\"margin-right: 10px;\">&#x25BA;</span>\n",
    "        <span>Expand to learn more about Whisper and its integration with MLflow.</span>\n",
    "    </summary>\n",
    "    <br/>\n",
    "    <div>\n",
    "        <h4>What is Whisper?</h4>\n",
    "        <p>Whisper, developed by OpenAI, is a versatile ASR model trained for high-accuracy speech-to-text conversion. It stands out due to its training on diverse accents and environments, available via the Transformers library for easy use.</p>\n",
    "    </div>\n",
    "    <div>\n",
    "        <h4>Why MLflow with Whisper?</h4>\n",
    "        <p>Integrating MLflow with Whisper enhances ASR model management:</p>\n",
    "        <ul>\n",
    "            <li><strong>Experiment Tracking</strong>: Facilitates tracking of model configurations and performance for optimal results.</li>\n",
    "            <li><strong>Model Management</strong>: Centralizes different versions of Whisper models, enhancing organization and accessibility.</li>\n",
    "            <li><strong>Reproducibility</strong>: Ensures consistency in transcriptions by tracking all components required for reproducing model behavior.</li>\n",
    "            <li><strong>Deployment</strong>: Streamlines the deployment of Whisper models in various production settings, ensuring efficient application.</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "</details>\n",
    "<br/>\n",
    "\n",
    "Interested in learning more about Whisper? To read more about the significant breakthroughs in transcription capabilities that Whisper brought to the field of ASR, you can [read the white paper](\"https://arxiv.org/abs/2212.04356\") and see more about the active development and [read more about the progress](\"https://openai.com/research/whisper\") at OpenAI's research website.\n",
    "\n",
    "Ready to enhance your speech-to-text capabilities? Let's explore automatic speech recognition using MLflow and Whisper!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up the Environment and Acquiring Audio Data\n",
    "\n",
    "Initial steps for transcription using [Whisper](\"https://github.com/openai/whisper\"): acquiring [audio](\"https://www.nasa.gov/audio-and-ringtones/\") and setting up MLflow.\n",
    "\n",
    "<details>\n",
    "    <summary style=\"cursor: pointer; display: flex; align-items: center;\">\n",
    "        <span style=\"margin-right: 10px;\">&#x25BA;</span>\n",
    "        <span>Expand for details on environment setup and audio data acquisition.</span>\n",
    "    </summary>\n",
    "    <br/>\n",
    "    <div>\n",
    "        <p>Before diving into the audio transcription process with OpenAI's Whisper, there are a few preparatory steps to ensure everything is in place for a smooth and effective transcription experience.</p>\n",
    "    </div>\n",
    "    <div>\n",
    "        <h4>Audio Acquisition</h4>\n",
    "        <p>The first step is to acquire an audio file to work with. For this tutorial, we use a publicly available audio file from NASA. This sample audio provides a practical example to demonstrate Whisper's transcription capabilities.</p>\n",
    "    </div>\n",
    "    <div>\n",
    "        <h4>Model and Pipeline Initialization</h4>\n",
    "        <p>We load the Whisper model, along with its tokenizer and feature extractor, from the Transformers library. These components are essential for processing the audio data and converting it into a format that the Whisper model can understand and transcribe.</p>\n",
    "        <p>Next, we create a transcription pipeline using the Whisper model. This pipeline simplifies the process of feeding audio data into the model and obtaining the transcription.</p>\n",
    "    </div>\n",
    "    <div>\n",
    "        <h4>MLflow Environment Setup</h4>\n",
    "        <p>In addition to the model and audio data setup, we initialize our MLflow environment. MLflow is used to track and manage our experiments, offering an organized way to document the transcription process and results.</p>\n",
    "    </div>\n",
    "    <div>\n",
    "        <p>The following code block covers these initial setup steps, providing the foundation for our audio transcription task with the Whisper model.</p>\n",
    "    </div>\n",
    "</details>\n",
    "<br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import transformers\n",
    "\n",
    "import mlflow\n",
    "\n",
    "\n",
    "# Acquire an audio file that is in the public domain\n",
    "resp = requests.get(\n",
    "    \"https://www.nasa.gov/wp-content/uploads/2015/01/590325main_ringtone_kennedy_WeChoose.mp3\"\n",
    ")\n",
    "resp.raise_for_status()\n",
    "audio = resp.content\n",
    "\n",
    "task = \"automatic-speech-recognition\"\n",
    "architecture = \"openai/whisper-large-v3\"\n",
    "\n",
    "model = transformers.WhisperForConditionalGeneration.from_pretrained(architecture)\n",
    "tokenizer = transformers.WhisperTokenizer.from_pretrained(architecture)\n",
    "feature_extractor = transformers.WhisperFeatureExtractor.from_pretrained(architecture)\n",
    "model.generation_config.alignment_heads = [[2, 2], [3, 0], [3, 2], [3, 3], [3, 4], [3, 5]]\n",
    "audio_transcription_pipeline = transformers.pipeline(\n",
    "    task=task, model=model, tokenizer=tokenizer, feature_extractor=feature_extractor\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting the Transcription Output\n",
    "\n",
    "In this section, we introduce a utility function that is used solely for the purpose of enhancing the readability of the transcription output within this Jupyter notebook demo. It is important to note that this function is designed for demonstration purposes and should not be included in production code or used for any other purpose beyond this tutorial.\n",
    "\n",
    "The `format_transcription` function takes a long string of transcribed text and formats it by splitting it into sentences and inserting newline characters. This makes the output easier to read when printed in the notebook environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_transcription(transcription):\n",
    "    \"\"\"\n",
    "    Function for formatting a long string by splitting into sentences and adding newlines.\n",
    "    \"\"\"\n",
    "    # Split the transcription into sentences, ensuring we don't split on abbreviations or initials\n",
    "    sentences = [\n",
    "        sentence.strip() + (\".\" if not sentence.endswith(\".\") else \"\")\n",
    "        for sentence in transcription.split(\". \")\n",
    "        if sentence\n",
    "    ]\n",
    "\n",
    "    # Join the sentences with a newline character\n",
    "    formatted_text = \"\\n\".join(sentences)\n",
    "\n",
    "    return formatted_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing the Transcription Pipeline\n",
    "\n",
    "Perform audio transcription using the Whisper pipeline and review the output.\n",
    "\n",
    "<details>\n",
    "    <summary style=\"cursor: pointer; display: flex; align-items: center;\">\n",
    "        <span style=\"margin-right: 10px;\">&#x25BA;</span>\n",
    "        <span>Expand to learn about the transcription process and its significance.</span>\n",
    "    </summary>\n",
    "    <br/>\n",
    "    <div>\n",
    "        <p>After setting up the Whisper model and audio transcription pipeline, our next step is to process an audio file to extract its transcription. This part of the tutorial is crucial as it demonstrates the practical application of the Whisper model in converting spoken language into written text.</p>\n",
    "    </div>\n",
    "    <div>\n",
    "        <h4>Transcription Process</h4>\n",
    "        <p>The code block below feeds an audio file into the pipeline, which then produces the transcription. The <code>format_transcription</code> function, defined earlier, enhances readability by formatting the output with sentence splits and newline characters.</p>\n",
    "    </div>\n",
    "    <div>\n",
    "        <h4>Importance of Pre-Save Testing</h4>\n",
    "        <p>Testing the transcription pipeline before saving the model in MLflow is vital. This step verifies that the model works as expected, ensuring accuracy and reliability. Such validation avoids issues post-deployment and confirms that the model performs consistently with the training data it was exposed to. It also provides a benchmark to compare against the output after the model is loaded back from MLflow, ensuring consistency in performance.</p>\n",
    "    </div>\n",
    "    <div>\n",
    "        <p>Execute the following code to transcribe the audio and assess the quality and accuracy of the transcription provided by the Whisper model.</p>\n",
    "    </div>\n",
    "</details>\n",
    "<br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We choose to go to the moon in this decade and do the other things.\n",
      "Not because they are easy, but because they are hard.\n",
      "3, 2, 1, 0.\n",
      "All engines running.\n",
      "Liftoff.\n",
      "We have a liftoff.\n",
      "32 minutes past the hour.\n",
      "Liftoff on Apollo 11.\n"
     ]
    }
   ],
   "source": [
    "transcription = audio_transcription_pipeline(audio)\n",
    "\n",
    "print(format_transcription(transcription[\"text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Signature and Configuration\n",
    "\n",
    "Generate a model signature for Whisper to understand its input and output data requirements.\n",
    "\n",
    "<details>\n",
    "    <summary style=\"cursor: pointer; display: flex; align-items: center;\">\n",
    "        <span style=\"margin-right: 10px;\">&#x25BA;</span>\n",
    "        <span>Expand to explore details on model signature and configuration.</span>\n",
    "    </summary>\n",
    "    <br/>\n",
    "    <div>\n",
    "        <p>The model signature is critical for defining the schema for the Whisper model's inputs and outputs, clarifying the data types and structures expected. This step ensures the model processes inputs correctly and outputs structured data.</p>\n",
    "    </div>\n",
    "    <div>\n",
    "        <h4>Handling Different Audio Formats</h4>\n",
    "        <p>While the default signature covers binary audio data, the <code>transformers</code> flavor accommodates multiple formats, including numpy arrays and URL-based inputs. This flexibility allows Whisper to transcribe from various sources, although URL-based transcription isn't demonstrated here.</p>\n",
    "    </div>\n",
    "    <div>\n",
    "        <h4>Model Configuration</h4>\n",
    "        <p>Setting the model configuration involves parameters like <i>chunk</i> and <i>stride</i> lengths for audio processing. These settings are adjustable to suit different transcription needs, enhancing Whisper's performance for specific scenarios.</p>\n",
    "    </div>\n",
    "    <div>\n",
    "        <p>Run the next code block to infer the model's signature and configure key parameters, aligning Whisper's functionality with your project's requirements.</p>\n",
    "    </div>\n",
    "</details>\n",
    "<br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"chunk_length_s\": 20,\n",
    "    \"stride_length_s\": [5, 3],\n",
    "}\n",
    "\n",
    "signature = mlflow.models.infer_signature(\n",
    "    audio,\n",
    "    mlflow.transformers.generate_signature_output(audio_transcription_pipeline, audio),\n",
    "    params=model_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the tracking server and creating an experiment\n",
    "\n",
    "In order to view the results in our tracking server (for the purposes of this tutorial, we've started a local tracking server at this url)\n",
    "\n",
    "We can start an instance of the MLflow server locally by running the following from a terminal to start the tracking server:\n",
    "\n",
    "``` bash\n",
    "    mlflow server --host 127.0.0.1 --port 8080\n",
    "```\n",
    "\n",
    "With the server started, the following code will ensure that all experiments, runs, models, parameters, and metrics that we log are being tracked within that server instance (which also provides us with the MLflow UI when navigating to that url address in a browser).\n",
    "\n",
    "After setting the tracking url, we create a new MLflow Experiment to store the run we're about to create in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/224567153691341564', creation_time=1699495264421, experiment_id='224567153691341564', last_update_time=1699495264421, lifecycle_stage='active', name='WhisperTranscription', tags={}>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(\"http://127.0.0.1:8080\")\n",
    "\n",
    "mlflow.set_experiment(\"WhisperTranscription\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging the Model with MLflow\n",
    "\n",
    "Learn how to log the Whisper model and its configurations with MLflow.\n",
    "\n",
    "<details>\n",
    "    <summary style=\"cursor: pointer; display: flex; align-items: center;\">\n",
    "        <span style=\"margin-right: 10px;\">&#x25BA;</span>\n",
    "        <span>Expand to learn about the process of logging models in MLflow.</span>\n",
    "    </summary>\n",
    "    <br/>\n",
    "    <div>\n",
    "        <p>Logging the Whisper model in MLflow is a critical step for capturing essential information for model reproduction, sharing, and deployment. This process involves:</p>\n",
    "    </div>\n",
    "    <div>\n",
    "        <h4>Key Components of Model Logging</h4>\n",
    "        <ul>\n",
    "            <li><strong>Model Information</strong>: Includes the model, its signature, and an input example.</li>\n",
    "            <li><strong>Model Configuration</strong>: Any specific parameters set for the model, like <i>chunk length</i> or <i>stride length</i>.</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "    <div>\n",
    "        <h4>Using MLflow's <code>log_model</code> Function</h4>\n",
    "        <p>This function is utilized within an MLflow run to log the model and its configurations. It ensures that all necessary components for model usage are recorded.</p>\n",
    "    </div>\n",
    "    <div>\n",
    "        <p>Executing the code in the next cell will log the Whisper model in the current MLflow experiment. This includes storing the model in a specified artifact path and documenting the default configurations that will be applied during inference.</p>\n",
    "    </div>\n",
    "</details>\n",
    "<br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamin.wilson/miniconda3/envs/mlflow-dev-env/lib/python3.8/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "# Log the pipeline\n",
    "with mlflow.start_run():\n",
    "    model_info = mlflow.transformers.log_model(\n",
    "        transformers_model=audio_transcription_pipeline,\n",
    "        artifact_path=\"whisper_transcriber\",\n",
    "        signature=signature,\n",
    "        input_example=audio,\n",
    "        model_config=model_config,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Using the Model Pipeline\n",
    "\n",
    "Explore how to load and use the Whisper model pipeline from MLflow.\n",
    "\n",
    "<details>\n",
    "    <summary style=\"cursor: pointer; display: flex; align-items: center;\">\n",
    "        <span style=\"margin-right: 10px;\">&#x25BA;</span>\n",
    "        <span>Expand to learn about loading and using the logged model pipeline.</span>\n",
    "    </summary>\n",
    "    <br/>\n",
    "    <div>\n",
    "        <p>After logging the Whisper model in MLflow, the next crucial step is to load and use it for inference. This process ensures that our logged model operates as intended and can be effectively used for tasks like audio transcription.</p>\n",
    "    </div>\n",
    "    <div>\n",
    "        <h4>Loading the Model</h4>\n",
    "        <p>The model is loaded in its native format using MLflow's <code>load_model</code> function. This step verifies that the model can be retrieved and used seamlessly after being logged in MLflow.</p>\n",
    "    </div>\n",
    "    <div>\n",
    "        <h4>Using the Loaded Model</h4>\n",
    "        <p>Once loaded, the model is ready for inference. We demonstrate this by passing an MP3 audio file to the model and obtaining its transcription. This test is a practical demonstration of the model's capabilities post-logging.</p>\n",
    "    </div>\n",
    "    <div>\n",
    "        <p>This step is a form of validation before moving to more complex deployment scenarios. Ensuring that the model functions correctly in its native format helps in troubleshooting and streamlines the deployment process, especially for large and complex models like Whisper.</p>\n",
    "    </div>\n",
    "</details>\n",
    "<br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44faa5e6467c4cbf8f689dedc834d91f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/11/08 22:53:55 INFO mlflow.store.artifact.artifact_repo: The progress bar can be disabled by setting the environment variable MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR to false\n",
      "2023/11/08 22:54:33 INFO mlflow.transformers: 'runs:/a77b9f3c037948228dd24787e33f91b4/whisper_transcriber' resolved as 'mlflow-artifacts:/224567153691341564/a77b9f3c037948228dd24787e33f91b4/artifacts/whisper_transcriber'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c3b32b7dd974aa691b870583fac8d37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a33d24662697426aa40dc13616a9f9aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Whisper native output transcription:\n",
      "We choose to go to the moon in this decade and do the other things.\n",
      "Not because they are easy, but because they are hard.\n",
      "3, 2, 1, 0.\n",
      "All engines running.\n",
      "Liftoff.\n",
      "We have a liftoff.\n",
      "32 minutes past the hour.\n",
      "Liftoff on Apollo 11.\n"
     ]
    }
   ],
   "source": [
    "# Load the pipeline in its native format\n",
    "loaded_transcriber = mlflow.transformers.load_model(model_uri=model_info.model_uri)\n",
    "\n",
    "transcription = loaded_transcriber(audio)\n",
    "\n",
    "print(f\"\\nWhisper native output transcription:\\n{format_transcription(transcription['text'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Pyfunc Flavor for Inference\n",
    "\n",
    "Learn how MLflow's `pyfunc` flavor facilitates flexible model deployment.\n",
    "\n",
    "<details>\n",
    "    <summary style=\"cursor: pointer; display: flex; align-items: center;\">\n",
    "        <span style=\"margin-right: 10px;\">&#x25BA;</span>\n",
    "        <span>Expand to discover the use of <code>pyfunc</code> for model inference.</span>\n",
    "    </summary>\n",
    "    <br/>\n",
    "    <div>\n",
    "        <p>MLflow's <code>pyfunc</code> flavor provides a generic interface for model inference, offering flexibility across various machine learning frameworks and deployment environments. This feature is beneficial for deploying models where the original framework may not be available, or a more adaptable interface is required.</p>\n",
    "    </div>\n",
    "    <div>\n",
    "        <h4>Loading and Predicting with Pyfunc</h4>\n",
    "        <p>The code below illustrates how to load the Whisper model as a <code>pyfunc</code> and use it for prediction. This method highlights MLflow's capability to adapt and deploy models in diverse scenarios.</p>\n",
    "    </div>\n",
    "    <div>\n",
    "        <h4>Output Format Considerations</h4>\n",
    "        <p>Note the difference in the output format when using <code>pyfunc</code> compared to the native format. The <code>pyfunc</code> output conforms to standard pyfunc output signatures, typically represented as a <code>List[str]</code> type, aligning with broader MLflow standards for model outputs.</p>\n",
    "    </div>\n",
    "</details>\n",
    "<br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f581e2c2050b422bbdc1cbabb403626d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/11/08 22:54:45 INFO mlflow.store.artifact.artifact_repo: The progress bar can be disabled by setting the environment variable MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR to false\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b94340e79ee4aa28aaaaa052d2f8d02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "2023/11/08 22:55:29 WARNING mlflow.transformers: params provided to the `predict` method will override the inference configuration saved with the model. If the params provided are not valid for the pipeline, MlflowException will be raised.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pyfunc output transcription:\n",
      "We choose to go to the moon in this decade and do the other things.\n",
      "Not because they are easy, but because they are hard.\n",
      "3, 2, 1, 0.\n",
      "All engines running.\n",
      "Liftoff.\n",
      "We have a liftoff.\n",
      "32 minutes past the hour.\n",
      "Liftoff on Apollo 11.\n"
     ]
    }
   ],
   "source": [
    "pyfunc_transcriber = mlflow.pyfunc.load_model(model_uri=model_info.model_uri)\n",
    "\n",
    "pyfunc_transcription = pyfunc_transcriber.predict([audio])\n",
    "\n",
    "# Note: the pyfunc return type if `return_timestamps` is set is a JSON encoded string.\n",
    "print(f\"\\nPyfunc output transcription:\\n{format_transcription(pyfunc_transcription[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial Roundup\n",
    "\n",
    "Throughout this tutorial, we've explored how to:\n",
    "\n",
    "- Set up an audio transcription pipeline using the OpenAI Whisper model.\n",
    "- Format and prepare audio data for transcription.\n",
    "- Log, load, and use the model with MLflow, leveraging both the native and pyfunc flavors for inference.\n",
    "- Format the output for readability and practical use in a Jupyter Notebook environment.\n",
    "\n",
    "We've seen the benefits of using MLflow for managing the machine learning lifecycle, including experiment tracking, model versioning, reproducibility, and deployment. By integrating MLflow with the Transformers library, we've streamlined the process of working with state-of-the-art NLP models, making it easier to track, manage, and deploy cutting-edge NLP applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlflow-dev-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
