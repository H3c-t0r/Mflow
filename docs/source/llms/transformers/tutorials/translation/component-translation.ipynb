{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Translation with Transformers and MLflow\n",
    "\n",
    "In this tutorial, we delve into the world of language translation by leveraging the power of [Transformers](https://huggingface.co/docs/transformers/model_doc/whisper) and MLflow. This guide is crafted for practitioners with a grasp of machine learning concepts who seek to streamline their translation model workflows. We will showcase the use of MLflow to log, manage, and serve a cutting-edge translation model - the `google/flan-t5-base` from the [ü§ó Hugging Face](https://huggingface.co/) library.\n",
    "\n",
    "### What is `flan-t5-base`?\n",
    "\n",
    "`flan-t5-base` is a versatile translation model developed by Google, capable of understanding and translating multiple languages. It is part of the T5 (Text-to-Text Transfer Transformer) models, designed to handle a variety of text-based tasks within one framework. This model is accessible through the Transformers library, which provides an array of pre-trained models for different natural language processing tasks.\n",
    "\n",
    "### Why MLflow with `flan-t5-base`?\n",
    "\n",
    "Combining MLflow with `flan-t5-base` offers numerous benefits:\n",
    "\n",
    "- **Experiment Tracking**: Monitor and benchmark different configurations and performance metrics of the translation model.\n",
    "- **Model Management**: Create a centralized model hub for various iterations of translation models along with their settings.\n",
    "- **Reproducibility**: Record all the details necessary to replicate a specific translation task.\n",
    "- **Deployment**: Streamline the process of deploying translation models into production settings.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "Throughout this tutorial, you will:\n",
    "\n",
    "- Construct a translation **pipeline** using `flan-t5-base` from the Transformers library.\n",
    "- **Log** the translation model and its configurations using MLflow.\n",
    "- Determine the input and output **signature** of the translation model automatically.\n",
    "- **Retrieve** a logged translation model from MLflow for direct interaction.\n",
    "- Emulate the deployment of the translation model using MLflow's **pyfunc** model flavor for language translation tasks.\n",
    "\n",
    "By the conclusion of this tutorial, you'll gain a thorough insight into managing and deploying translation models with MLflow, thereby enhancing your machine learning operations for language processing.\n",
    "\n",
    "Let's embark on this journey of language translation with MLflow and `flan-t5-base`!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up the Translation Environment\n",
    "\n",
    "Before diving into the translation tasks, we need to set up our environment. This involves importing the necessary libraries and initializing the translation model and tokenizer. We will be using the `google/flan-t5-base` model, which is a part of the T5 family known for its effectiveness in translation tasks.\n",
    "\n",
    "The following steps will be covered in this setup:\n",
    "\n",
    "1. **Importing Libraries**: We will import the `transformers` library, which provides us with the translation model and tokenizer, as well as `mlflow` for model tracking and management.\n",
    "2. **Initializing the Model**: The `google/flan-t5-base` model will be loaded from the Hugging Face model repository. This model has been pre-trained and is ready for translation tasks.\n",
    "3. **Setting Up the Tokenizer**: The corresponding tokenizer for our model will be initialized. The tokenizer is responsible for converting text input into a format that the model can understand.\n",
    "4. **Creating the Pipeline**: We will create a pipeline for translation from English to French. This pipeline abstracts away the model and tokenizer interaction, allowing us to directly input text and receive the translation.\n",
    "\n",
    "With these components in place, we will be able to perform translations seamlessly.\n",
    "\n",
    "Let's proceed to initialize our translation environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "import mlflow\n",
    "\n",
    "model_architecture = \"google/flan-t5-base\"\n",
    "\n",
    "translation_pipeline = transformers.pipeline(\n",
    "    task=\"translation_en_to_fr\",\n",
    "    model=transformers.T5ForConditionalGeneration.from_pretrained(\n",
    "        model_architecture, max_length=1000\n",
    "    ),\n",
    "    tokenizer=transformers.T5TokenizerFast.from_pretrained(model_architecture, return_tensors=\"pt\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Translation Pipeline\n",
    "\n",
    "Before we proceed to log our model with MLflow, it's crucial to verify that our translation pipeline is functioning correctly. Given the substantial size of these models, it's best practice to ensure that the base model from the library performs as expected. This step helps in avoiding the time-consuming process of saving and then troubleshooting a model that may have issues during inference when loaded back from MLflow.\n",
    "\n",
    "Here's why this preliminary check is important:\n",
    "\n",
    "- **Model Verification**: By running a test translation, we can confirm that the model correctly translates text from English to French.\n",
    "- **Error Prevention**: Identifying any issues early on, before logging the model, can save us from future errors that might arise during model deployment or inference.\n",
    "- **Resource Management**: Large models require significant resources to save and load. Testing the model beforehand ensures that we are using our resources efficiently.\n",
    "- **Pipeline Validation**: This step also validates that the pipeline, which includes the model and tokenizer, is set up correctly and can process input data as intended.\n",
    "\n",
    "By conducting a test translation, we can confidently proceed to log our model with MLflow, knowing that the underlying model is functioning properly. Let's execute a translation to verify our setup:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': \"J'ai appr√©ci√© mon sajour lente sur les Champs-√âlys√©es.\"}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation_pipeline(\n",
    "    \"translate English to French: I enjoyed my slow saunter along the Champs-√âlys√©es.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Translation Results\n",
    "\n",
    "Upon running our initial translation through the pipeline, we received the following output:\n",
    "\n",
    "```text\n",
    "[{'translation_text': \"J'ai appr√©ci√© mon sajour lente sur les Champs-√âlys√©es.\"}]\n",
    "```\n",
    "\n",
    "While this translation captures the essence of the original English sentence, it does exhibit a few areas where accuracy could be improved. Notably, there are minor grammatical errors and the choice of words could be refined. However, considering the complexity of language translation, especially with nuances and context, the model has done a commendable job on its first attempt.\n",
    "\n",
    "The translation demonstrates the model's ability to grasp the core meaning of the text and convert it into a structurally similar sentence in French. This is a testament to the power of the underlying machine learning model and its training on diverse linguistic data.\n",
    "\n",
    "To put it into perspective, a more polished translation might look like this:\n",
    "\n",
    "```text\n",
    "\"J'ai appr√©ci√© ma lente promenade le long des Champs-√âlys√©es.\"\n",
    "```\n",
    "\n",
    "This refined version corrects the grammatical gender of 's√©jour' to 'promenade' and adds the necessary article 'des' to 'Champs-√âlys√©es', along with the correct accentuation and hyphenation. It's these subtle nuances that transform a good translation into a great one.\n",
    "\n",
    "It's encouraging to see that with just a base model, we are already close to a natural and accurate translation. With further fine-tuning and possibly more context provided to the model, we can expect even more precise translations. This initial result is promising and indicates that our pipeline is well-configured and ready for more advanced use cases and optimizations.\n",
    "\n",
    "In the world of machine translation, perfection is an ongoing pursuit, and this result is a solid stepping stone towards that goal. Let's celebrate the successes of our model and look forward to the improvements that iterative development and learning will bring. Thankfully, MLflow will have you completely covered if you should choose to improve the results of such a model by helping you keep track of the iterative fine tuning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Model Parameters and Inferring Signature\n",
    "\n",
    "Before we proceed to log our model with MLflow, it's crucial to define the model parameters and infer the model signature. The model parameters dictate how our model behaves during inference. For instance, setting max_length to 1000 specifies the maximum length of the sequence to be generated. This ensures that our model can handle longer sentences without truncating them prematurely, which is essential for maintaining the context and meaning of the translation.\n",
    "\n",
    "Inferring the model signature is also a pivotal step. The signature represents the schema of the model's inputs and outputs, allowing MLflow to understand the data types and structures that the model expects and produces. By providing a sample input and generating a corresponding output, we enable MLflow to capture this information, which aids in ensuring consistency and reliability when the model is deployed in different environments.\n",
    "\n",
    "This process is a best practice that enhances model portability and reduces the risk of schema-related errors during deployment. It also provides clear documentation for developers and users of the model, making it easier to integrate the model into downstream applications.\n",
    "\n",
    "By setting the model parameters and inferring the signature before logging the model, we establish a solid foundation for tracking, managing, and serving our translation model with confidence. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\"max_length\": 1000}\n",
    "\n",
    "signature = mlflow.models.infer_signature(\n",
    "    \"This is a sample input sentence.\",\n",
    "    mlflow.transformers.generate_signature_output(translation_pipeline, \"This is another sample.\"),\n",
    "    params=model_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reviewing the Model Signature\n",
    "\n",
    "After setting the parameters and inferring the signature for our translation model, it's important to review the signature to ensure that it accurately reflects the input and output data structures. The signature is a blueprint that MLflow uses to understand how to interact with the model. It specifies the types of data the model expects as input and predicts as output, as well as any additional parameters that control the model's behavior.\n",
    "\n",
    "Here's what each part of the signature represents:\n",
    "\n",
    "- **Inputs**: This section lists the expected input data type(s). In our case, the model expects a string input, which corresponds to the text we want to translate.\n",
    "\n",
    "- **Outputs**: This outlines the data type(s) of the model's output. For our translation model, the output is also a string, representing the translated text.\n",
    "\n",
    "- **Parameters**: These are the additional settings that can be configured for the model. The `max_length` parameter shown here is set to a long integer with a default value of 1000, indicating the maximum length of the generated translation.\n",
    "\n",
    "By executing the `signature` command, we can visually confirm that the signature matches our expectations. This is a form of validation to ensure that when we deploy the model, it will receive and produce data in the format we have defined. It's a crucial step for model deployment and serves as a contract that guarantees the model's inputs and outputs remain consistent, avoiding potential runtime errors in production.\n",
    "\n",
    "Additionally, it's worth noting that if we wish to override a parameter for the model at inference time, we must declare it with a default value at the time of signature generation. This ensures that our model's behavior is predictable and that any changes to its configuration are intentional and documented.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inputs: \n",
       "  [string]\n",
       "outputs: \n",
       "  [string]\n",
       "params: \n",
       "  ['max_length': long (default: 1000)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the tracking server and creating an experiment\n",
    "\n",
    "In order to view the results in our tracking server (for the purposes of this tutorial, we've started a local tracking server at this url)\n",
    "\n",
    "We can start an instance of the MLflow server locally by running the following from a terminal to start the tracking server:\n",
    "\n",
    "``` bash\n",
    "    mlflow server --host 127.0.0.1 --port 8080\n",
    "```\n",
    "\n",
    "With the server started, the following code will ensure that all experiments, runs, models, parameters, and metrics that we log are being tracked within that server instance (which also provides us with the MLflow UI when navigating to that url address in a browser).\n",
    "\n",
    "After setting the tracking url, we create a new MLflow Experiment to store the run we're about to create in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/390523574656024103', creation_time=1699572826948, experiment_id='390523574656024103', last_update_time=1699572826948, lifecycle_stage='active', name='Translation', tags={}>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(\"http://127.0.0.1:8080\")\n",
    "\n",
    "mlflow.set_experiment(\"Translation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging the Model with MLflow\n",
    "\n",
    "Once we have verified the functionality of our translation model and confirmed the signature, the next step is to log the model with MLflow. This process involves starting an MLflow run and using the `mlflow.transformers.log_model` function to save our model. Here's what each argument in the function call is doing:\n",
    "\n",
    "- `transformers_model`: This is the actual translation model pipeline that we have created and tested. It includes both the model and tokenizer, and it's ready to be logged for tracking and versioning.\n",
    "\n",
    "- `artifact_path`: This is the directory within the MLflow run where the model artifacts will be stored. In this case, we're naming it \"french_translator\".\n",
    "\n",
    "- `signature`: This is the model signature we previously generated and reviewed. It ensures that MLflow knows the expected input and output formats for the model.\n",
    "\n",
    "- `model_params`: These are the parameters of the model that we want to log. In our case, we're logging the `max_length` parameter, which we've set to 1000.\n",
    "\n",
    "By wrapping this function call within a `with mlflow.start_run()` block, we're creating a new MLflow run. This run acts as a container for all the information we log about this model, including the model itself, its parameters, and its signature. Once the model is logged, we receive a `model_info` object, which contains metadata about the logged model, including the location where it's stored. This information is crucial for later stages when we want to deploy the model or analyze its performance across different runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/Users/benjamin.wilson/miniconda3/envs/mlflow-dev-env/lib/python3.8/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run():\n",
    "    model_info = mlflow.transformers.log_model(\n",
    "        transformers_model=translation_pipeline,\n",
    "        artifact_path=\"french_translator\",\n",
    "        signature=signature,\n",
    "        model_params=model_params,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the Loaded Model Components\n",
    "\n",
    "Once the model is loaded using MLflow, we can inspect the individual components to confirm their integrity and types. The output from the previous cell provides us with a clear picture of what components are available:\n",
    "\n",
    "- `task`: A string indicating the type of task the model is intended for.\n",
    "- `device_map`: A string that represents the device mapping if the model has been configured to run on specific hardware.\n",
    "- `model`: An instance of `T5ForConditionalGeneration`, which is the core of the translation model.\n",
    "- `tokenizer`: The `T5TokenizerFast` object used to preprocess text for the model.\n",
    "- `framework`: A string that indicates the deep learning framework used by the model.\n",
    "\n",
    "This information is crucial for understanding how the model operates and ensures that each component is correctly loaded and identified. It also confirms that the model can be reconstructed for inference, further training, or analysis, maintaining the flexibility and robustness of the MLflow platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4b1eb831c7640368f07a8738c5954f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2023/11/09 22:17:16 INFO mlflow.store.artifact.artifact_repo: The progress bar can be disabled by setting the environment variable MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR to false\n",
      "2023/11/09 22:17:28 INFO mlflow.transformers: 'runs:/50aa1efbd8354d788942b3fe587b6d01/french_translator' resolved as 'mlflow-artifacts:/390523574656024103/50aa1efbd8354d788942b3fe587b6d01/artifacts/french_translator'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9d5ad88eae74c7384577a3a0ef85988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2be2f948412b415cb90db757a7d38e97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task -> str\n",
      "device_map -> str\n",
      "model -> T5ForConditionalGeneration\n",
      "tokenizer -> T5TokenizerFast\n",
      "framework -> str\n"
     ]
    }
   ],
   "source": [
    "translation_components = mlflow.transformers.load_model(\n",
    "    model_info.model_uri, return_type=\"components\"\n",
    ")\n",
    "\n",
    "for key, value in translation_components.items():\n",
    "    print(f\"{key} -> {type(value).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Model Flavors in MLflow\n",
    "\n",
    "The `model_info.flavors` attribute provides a detailed description of the different \"flavors\" that MLflow uses to manage and deploy the model. Flavors are a way to abstract the model's capabilities and requirements, making it easier to deploy across different platforms. Here's what each key in the output dictionary represents:\n",
    "\n",
    "- `python_function`: This flavor indicates that the model can be loaded as a generic Python function. It includes:\n",
    "  - `model_binary`: The path to the binary model file.\n",
    "  - `loader_module`: The module used by MLflow to load the model.\n",
    "  - `python_version`: The version of Python with which the model is compatible.\n",
    "  - `env`: The environment specifications, including both conda and virtualenv options.\n",
    "\n",
    "- `transformers`: This flavor is specific to models from the Hugging Face Transformers library. It includes:\n",
    "  - `transformers_version`: The version of the Transformers library used.\n",
    "  - `code`: Any additional code dependencies required by the model.\n",
    "  - `task`: The specific task the model is trained for, in this case, `translation_en_to_fr`.\n",
    "  - `instance_type`: The class of the pipeline instance, here `TranslationPipeline`.\n",
    "  - `source_model_name`: The name of the pre-trained model used, `google/flan-t5-base`.\n",
    "  - `pipeline_model_type`: The type of model, `T5ForConditionalGeneration`.\n",
    "  - `framework`: The deep learning framework, PyTorch in this case (`pt`).\n",
    "  - `tokenizer_type`: The type of tokenizer used, `T5TokenizerFast`.\n",
    "  - `components`: The list of components included in the model, which for this model is the tokenizer.\n",
    "  - `model_binary`: The path to the binary model file.\n",
    "\n",
    "This information is essential for understanding how to interact with the model within the MLflow ecosystem, ensuring that the correct environment and dependencies are used when deploying the model for inference or further training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'python_function': {'model_binary': 'model',\n",
       "  'loader_module': 'mlflow.transformers',\n",
       "  'python_version': '3.8.13',\n",
       "  'env': {'conda': 'conda.yaml', 'virtualenv': 'python_env.yaml'}},\n",
       " 'transformers': {'transformers_version': '4.34.1',\n",
       "  'code': None,\n",
       "  'task': 'translation_en_to_fr',\n",
       "  'instance_type': 'TranslationPipeline',\n",
       "  'source_model_name': 'google/flan-t5-base',\n",
       "  'pipeline_model_type': 'T5ForConditionalGeneration',\n",
       "  'framework': 'pt',\n",
       "  'tokenizer_type': 'T5TokenizerFast',\n",
       "  'components': ['tokenizer'],\n",
       "  'model_binary': 'model'}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_info.flavors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Translation Output\n",
    "\n",
    "We're now going to test the loaded pipeline with a somewhat challenging sentence, loaded via the native default process, which will return a pipeline instance. \n",
    "\n",
    "The result below is quite satisfactory as the model has correctly identified \"Nice\" as the proper noun referring to the city, rather than just an adjective. Moreover, it has adeptly navigated the play on words by choosing an appropriate French adjective \"bien\" to convey the sentiment that Nice is a pleasant place to be during this time of the year.\n",
    "\n",
    "Such nuances in translation from English to French demonstrate the model's capability to understand context and the subtleties of language. This is a positive indication of the model's utility for real-world applications where accurate and context-aware translations are necessary.\n",
    "\n",
    "It's also a reminder of the importance of testing models with sentences that have multiple meanings or interpretations to ensure that the model can handle a variety of linguistic challenges.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ac046b05ff14014b6b62d211cdf5d1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/11/09 22:17:29 INFO mlflow.store.artifact.artifact_repo: The progress bar can be disabled by setting the environment variable MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR to false\n",
      "2023/11/09 22:17:38 INFO mlflow.transformers: 'runs:/50aa1efbd8354d788942b3fe587b6d01/french_translator' resolved as 'mlflow-artifacts:/390523574656024103/50aa1efbd8354d788942b3fe587b6d01/artifacts/french_translator'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c27215b5c1e74583bfe0c4a3addc925a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4dc22175b0b4a5a8f480d03bd57e4a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': \"J'ai entendu que Nice est bien cette p√©riode de l'ann√©e.\"}]\n"
     ]
    }
   ],
   "source": [
    "translation_pipeline = mlflow.transformers.load_model(model_info.model_uri)\n",
    "response = translation_pipeline(\"I have heard that Nice is nice this time of year.\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing the Reconstructed Pipeline's Translation\n",
    "In this next section, we're going to take the components that we loaded (a dictionary consisting of the required elements to perform inference) and reconstruct a new pipeline from these components. \n",
    "\n",
    "As you can see in the next section, the reconstructed pipeline successfully translated the English input into French, capturing the essence of how the Transformers library simplifies the use of deep learning models. The translation is not only syntactically correct but also semantically coherent, reflecting the original sentence's positive tone about the ease and enjoyment of using deep learning models.\n",
    "\n",
    "This test confirms that our pipeline components have been correctly logged and retrieved from MLflow, and that the reconstructed pipeline is functioning as expected. It's a crucial step to ensure that the model we've trained and the components we've saved can be used effectively after being deployed, maintaining the integrity of the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': \"transformers simplifie l'utilisation des mod√®les de l'apprentissage profonde!\"}]\n"
     ]
    }
   ],
   "source": [
    "reconstructed_pipeline = transformers.pipeline(**translation_components)\n",
    "\n",
    "reconstructed_response = reconstructed_pipeline(\n",
    "    \"transformers makes using Deep Learning models easy and fun!\"\n",
    ")\n",
    "\n",
    "print(reconstructed_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct Utilization of Model Components\n",
    "\n",
    "In addition to using the full pipeline, we have the flexibility to interact with individual components of the model. This can be particularly useful for customizing the translation process or integrating the model into a larger system where you might need to manipulate the inputs and outputs more directly.\n",
    "\n",
    "By examining the keys of the translation_components dictionary, we gain insight into the structure of our model and the available components. This includes the task specification, device mapping, the core model itself, the tokenizer responsible for preparing our inputs, and the framework information. Each component plays a vital role in the translation process and can be utilized independently for a more granular level of control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['task', 'device_map', 'model', 'tokenizer', 'framework'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation_components.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Usage: Direct Interaction with Model Components\n",
    "\n",
    "While the pipeline approach offers a convenient and high-level interface for translations, direct interaction with the model's components can be beneficial in certain scenarios. This method requires a deeper understanding of the model and tokenizer but provides the opportunity to insert custom logic at various points in the process, offering greater flexibility.\n",
    "\n",
    "In the following code block, we manually handle the translation process by directly using the tokenizer and model components. This allows us to:\n",
    "\n",
    "- Customize the tokenization process.\n",
    "- Modify the tensor handling, such as specifying the device (CPU, GPU, MPS, etc.).\n",
    "- Generate predictions with the possibility of adjusting parameters on-the-fly.\n",
    "- Decode the outputs with the option to post-process the results.\n",
    "\n",
    "This granular control can be crucial for advanced use cases where you need to intervene in the model's operations, such as adjusting the inputs based on dynamic conditions or post-processing the model's outputs before presenting them to the end-user.\n",
    "\n",
    "However, this flexibility comes at the cost of increased complexity. Unlike the pipeline, which abstracts away many of the underlying operations, using the components directly requires managing more code and understanding the intricacies of the model's behavior. It's a trade-off between ease of use and control that needs to be considered based on the specific requirements of your application.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "La libert√©, l'√©galit√©, la fraternit√© ou la mort.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = translation_components[\"tokenizer\"]\n",
    "model = translation_components[\"model\"]\n",
    "\n",
    "query = \"Translate to French: Liberty, equality, fraternity, or death.\"\n",
    "\n",
    "# This notebook was run on a Mac laptop, so we'll send the output tensor to the \"mps\" device.\n",
    "# If you're running this on a different system, ensure that you're sending the tensor output to the appropriate device to ensure that\n",
    "# the model is able to read it from memory.\n",
    "inputs = tokenizer.encode(query, return_tensors=\"pt\").to(\"mps\")\n",
    "outputs = model.generate(inputs).to(\"mps\")\n",
    "result = tokenizer.decode(outputs[0])\n",
    "\n",
    "# Since we're not using a pipeline here, we need to modify the output slightly to get only the translated text.\n",
    "print(result.replace(\"<pad> \", \"\\n\").replace(\"</s>\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection on the Translation Output\n",
    "\n",
    "Upon examining the final translation output, we observe that it is very close to the iconic French motto: \"Libert√©, √©galit√©, fraternit√©, ou la mort.\" While the translation is not exact, it captures the essence of the phrase, demonstrating the model's capability to convey the meaning of complex and historically significant sentences. This slight deviation underscores the importance of context and cultural knowledge in language models.\n",
    "\n",
    "The phrase \"Libert√©, √©galit√©, fraternit√©\" is more than just a collection of words; it is an emblematic slogan of the French Republic, a symbol of its values and history. This highlights an area where even advanced models like the one we've used can benefit from further refinement and contextual awareness. For more on this phrase and its significance, you can explore its [Wikipedia page](https://en.wikipedia.org/wiki/Libert%C3%A9,_%C3%A9galit%C3%A9,_fraternit%C3%A9)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial Recap\n",
    "\n",
    "Throughout this tutorial, we've delved into the integration of MLflow with a state-of-the-art language model for translation tasks. We've covered a lot of ground, including:\n",
    "\n",
    "- Setting up and testing a translation pipeline.\n",
    "- Logging the model and its parameters to MLflow.\n",
    "- Inferring and understanding the model's signature.\n",
    "- Loading and interacting with the model components for greater flexibility.\n",
    "- Reflecting on the nuances of language translation and the importance of context.\n",
    "\n",
    "#### The Power of MLflow and Model Metadata\n",
    "\n",
    "The use of MLflow in this tutorial has demonstrated how it can streamline the process of managing and deploying machine learning models. By logging models with their parameters and metadata, MLflow ensures that we have a robust system for tracking experiments, managing model versions, and simplifying deployment.\n",
    "\n",
    "The metadata stored within MLflow, such as the model's signature and components, plays a crucial role in ensuring that models are deployed consistently and reliably. It provides valuable information about the expected inputs and outputs, making it easier to integrate the model into production systems.\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "As we conclude this tutorial, it's clear that the combination of powerful language models and robust MLOps tools like MLflow can significantly enhance our ability to deploy sophisticated AI solutions. Whether you're working on translation, speech recognition, or any other machine learning task, the principles we've explored here will help you to manage and deploy your models with confidence and precision.\n",
    "\n",
    "Thank you for joining me on this journey through automatic language translation and model management. I hope you've found it informative and empowering as you continue to work with these incredible tools in your own projects."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlflow-dev-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
