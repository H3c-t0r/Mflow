{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and Serving an OpenAI-compatible Chatbot\n",
    "\n",
    "Welcome to our tutorial on using Transformers and MLflow to create an OpenAI-compatible chat model. In Mlflow 2.11 and up, the [ChatModel](https://mlflow.org/docs/latest/python_api/mlflow.types.html#) class has been added, allowing for convenient creation of served models that conform to the OpenAI API spec. This enables you to seamlessly swap out your chat app's backing LLM, or to easily evaluate different models without having to edit your client-side code.\n",
    "\n",
    "If you haven't already seen it, you may find it helpful to go through our [introductory notebook on chat and Transformers](https://mlflow.org/docs/latest/llms/transformers/tutorials/conversational/conversational-model.html) before proceeding with this one, as this notebook is slightly higher-level and does not delve too deeply into the inner workings of Transformers or MLflow Tracking.\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "In this tutorial, you will:\n",
    "\n",
    "- Create an OpenAI-compatible chat model using TinyLLama-1.1B-Chat\n",
    "- Serve the model with MLflow Model Serving\n",
    "- Learn how to use MLflow's `pyfunc` API to add arbitrary customization to your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=false\n"
     ]
    }
   ],
   "source": [
    "# Disable tokenizers warnings when constructing pipelines\n",
    "%env TOKENIZERS_PARALLELISM=false\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Disable a few less-than-useful UserWarnings from setuptools and pydantic\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serving Chat Model with Transformers\n",
    "\n",
    "MLflow's native Transformers integration allows you to specify the `task` param when saving or logging your `text-generation` pipelines. This param can be one of two string literals, conforming to the [MLflow Deployments Server's endpoint_type specification](https://mlflow.org/docs/latest/llms/deployments/index.html#general-configuration-parameters) (\"llm/v1/embeddings\" can be specified as a task on models saved with `mlflow.sentence_transformers`):\n",
    "\n",
    "- \"llm/v1/chat\" for chat-style applications\n",
    "- \"llm/v1/completions\" for generic completions\n",
    "\n",
    "When the `task` param is specified, MLflow will automatically handle everything required to serve a chat model. This includes:\n",
    "\n",
    "- Setting a chat-compatible signature on the model\n",
    "- Performing data pre- and post-processing to ensure the inputs and outputs conform to the [Chat API spec](https://mlflow.org/docs/latest/llms/deployments/index.html#chat), which is compatible with OpenAI's API spec.\n",
    "\n",
    "Note that these modifications only apply when the model is loaded with `mlflow.pyfunc.load_model()` (e.g. when serving the model with the `mlflow models serve` CLI tool). If you want to load just the base pipeline, you can always do so via `mlflow.transformers.load_model()`.\n",
    "\n",
    "Here's an example of how to save a Transformers model with the `task` param using TinyLlama-1.1B-Chat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qd/9rwd0_gd0qs65g4sdqlm51hr0000gp/T/ipykernel_74928/2826724314.py:10: FutureWarning: The 'transformers' MLflow Models integration is known to be compatible with the following package version ranges: ``4.25.1`` -  ``4.37.1``. MLflow Models integrations with transformers may not succeed when used with package versions outside of this range.\n",
      "  mlflow.transformers.save_model(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "import mlflow\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    ")\n",
    "\n",
    "mlflow.transformers.save_model(\n",
    "    transformers_model=generator, path=\"tinyllama-chat\", task=\"llm/v1/chat\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is saved, we simply have to serve the model using the `mlflow models serve` CLI tool. \n",
    "\n",
    "In a terminal shell, run:\n",
    "```\n",
    "$ mlflow models serve -m tinyllama-chat\n",
    "```\n",
    "\n",
    "When the server has finished initializing, you should be able to interact with the model via HTTP requests. The input format is almost identical to the format described in the [Mlflow Deployments Server docs](https://mlflow.org/docs/latest/llms/deployments/index.html#chat), with the exception that `temperature` defaults to `1.0` instead of `0.0`.\n",
    "\n",
    "Here's a quick example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   706  100   617  100    89     30      4  0:00:22  0:00:20  0:00:02   191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"id\": \"45f7b834-07d8-42bf-bae0-35d21d0578f8\",\n",
      "    \"object\": \"chat.completion\",\n",
      "    \"created\": 1708918105,\n",
      "    \"model\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
      "    \"usage\": {\n",
      "      \"prompt_tokens\": 24,\n",
      "      \"completion_tokens\": 71,\n",
      "      \"total_tokens\": 95\n",
      "    },\n",
      "    \"choices\": [\n",
      "      {\n",
      "        \"index\": 0,\n",
      "        \"finish_reason\": \"stop\",\n",
      "        \"message\": {\n",
      "          \"role\": \"assistant\",\n",
      "          \"content\": \"Here's a simple hello world program in Python:\\n\\n```python\\nprint(\\\"Hello, world!\\\")\\n```\\n\\nThis program prints the string \\\"Hello, world!\\\" to the console. You can run this program by typing it into the Python interpreter or by running the command `python hello_world.py` in your terminal.\"\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "curl http://127.0.0.1:5000/invocations \\\n",
    "    -H 'Content-Type: application/json' \\\n",
    "    -d '{ \"messages\": [{\"role\": \"user\", \"content\": \"Write me a hello world program in python\"}] }' \\\n",
    "    | jq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's that easy!\n",
    "\n",
    "You can also call the API with a few optional inference params to adjust the model's responses. These map to Transformers pipeline params, and are passed in directly at inference time.\n",
    "\n",
    "- `max_tokens` (maps to `max_new_tokens`): The maximum number of new tokens the model should generate.\n",
    "- `temperature` (maps to `temperature`): Controls the creativity of the model's response. Note that this is not guaranteed to be supported by all models, and in order for this param to have an effect, the pipeline must have been created with `do_sample=True`.\n",
    "- `stop` (maps to `stopping_criteria`): A list of tokens at which to stop generation.\n",
    "\n",
    "Note: `n` does not have an equivalent Transformers pipeline param, and is not supported in queries. However, you can implement a model that consumes the `n` param using Custom Pyfunc (details below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customizing the model\n",
    "\n",
    "As always, custom functionality can be achieved with Mlflow's `pyfunc` API. In the cell below, we create a custom Chat-flavored `pyfunc` model by subclassing `mlflow.pyfunc.ChatModel`. We use our previously-saved TinyLlama pipeline as the backing model by loading it with `mlflow.transformers.load_model()`, and then build our own customizations on top of it.\n",
    "\n",
    "The possibilities are endless here, but as a quick example, the code below simply edits the ID of the response, rather than having it be a random UUID. Of course, you could also insert any side-effects you wanted here, such as asynchronously logging some metadata for analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import mlflow\n",
    "from mlflow.types.llm import ChatResponse\n",
    "\n",
    "\n",
    "class MyChatModel(mlflow.pyfunc.ChatModel):\n",
    "    def load_context(self, context):\n",
    "        # load our previously-saved Transformers pipeline from context.artifacts\n",
    "        self.pipeline = mlflow.transformers.load_model(context.artifacts[\"chat_model_path\"])\n",
    "\n",
    "    def predict(self, context, messages, params):\n",
    "        tokenizer = self.pipeline.tokenizer\n",
    "        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "        # perform inference using the loaded pipeline\n",
    "        output = self.pipeline(prompt, return_full_text=False, generation_kwargs=params.to_dict())\n",
    "        text = output[0][\"generated_text\"]\n",
    "        id = f\"some_meaningful_id_{random.randint(0, 100)}\"\n",
    "\n",
    "        # construct token usage information\n",
    "        prompt_tokens = len(tokenizer.encode(prompt))\n",
    "        completion_tokens = len(tokenizer.encode(text))\n",
    "        usage = {\n",
    "            \"prompt_tokens\": prompt_tokens,\n",
    "            \"completion_tokens\": completion_tokens,\n",
    "            \"total_tokens\": prompt_tokens + completion_tokens,\n",
    "        }\n",
    "\n",
    "        # here, we can do any post-processing or side-effects required.\n",
    "        # for example, we could log the generated text to a database for\n",
    "        # analytics, or check the output for any banned words or phrases\n",
    "        # and return a different response if any are found.\n",
    "\n",
    "        # in this example, we just return the generated text as the response\n",
    "\n",
    "        response = {\n",
    "            \"id\": id,\n",
    "            \"model\": \"MyChatModel\",\n",
    "            \"choices\": [\n",
    "                {\n",
    "                    \"index\": 0,\n",
    "                    \"message\": {\"role\": \"assistant\", \"content\": text},\n",
    "                    \"finish_reason\": \"stop\",\n",
    "                }\n",
    "            ],\n",
    "            \"usage\": usage,\n",
    "        }\n",
    "\n",
    "        return ChatResponse(**response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to what happens above, upon saving an instance of `MyChatModel`, MLflow will automatically recognize the `mlflow.pyfunc.ChatModel` subclass, and set chat signatures and handle input and output parsing automatically. Note that enforcement is performed on the output--MLflow will run inference on an example input, and assert that the output is of type `ChatResponse`.\n",
    "\n",
    "Full documentation for the `ChatResponse` type can be found in the [API reference](https://mlflow.org/docs/latest/python_api/mlflow.types.html#mlflow.types.llm.ChatResponse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/02/26 12:30:53 INFO mlflow.pyfunc: Predicting on input example to validate output\n",
      "/var/folders/qd/9rwd0_gd0qs65g4sdqlm51hr0000gp/T/ipykernel_74928/2688517701.py:9: FutureWarning: The 'transformers' MLflow Models integration is known to be compatible with the following package version ranges: ``4.25.1`` -  ``4.37.1``. MLflow Models integrations with transformers may not succeed when used with package versions outside of this range.\n",
      "  self.pipeline = mlflow.transformers.load_model(context.artifacts[\"chat_model_path\"])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00d4099957e6463b8cbbe40274ee3043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/02/26 12:30:53 WARNING mlflow.transformers: Could not specify device parameter for this pipeline type\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06e7fa59957c42f3a2c114557e1c12b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13fae1510ce74cb5be9cb8e192350ea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/02/26 12:31:23 INFO mlflow.store.artifact.artifact_repo: The progress bar can be disabled by setting the environment variable MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR to false\n"
     ]
    }
   ],
   "source": [
    "mlflow.pyfunc.save_model(\n",
    "    path=\"my_model\",\n",
    "    python_model=MyChatModel(),\n",
    "    # provide the path to the pipeline we saved earlier\n",
    "    artifacts={\"chat_model_path\": \"tinyllama-chat\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can now serve the model by running the following in a terminal shell:\n",
    "\n",
    "```\n",
    "$ mlflow models serve -m my_model\n",
    "```\n",
    "\n",
    "And we should now be able to query it via HTTP request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   666  100   577  100    89     20      3  0:00:29  0:00:28  0:00:01   171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"some_meaningful_id_21\",\n",
      "  \"model\": \"MyChatModel\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Here's a simple hello world program in Python:\\n\\n```python\\nprint(\\\"Hello, world!\\\")\\n```\\n\\nThis program prints the string \\\"Hello, world!\\\" to the console. You can run this program by typing it into the Python interpreter or by running the command `python hello_world.py` in your terminal.\"\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 25,\n",
      "    \"completion_tokens\": 71,\n",
      "    \"total_tokens\": 96\n",
      "  },\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1708918478\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "curl http://127.0.0.1:5000/invocations \\\n",
    "    -H 'Content-Type: application/json' \\\n",
    "    -d '{ \"messages\": [{\"role\": \"user\", \"content\": \"Write me a hello world program in python\"}] }' \\\n",
    "    | jq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, you learned how to create an OpenAI-compatible chat model by specifying the `task` parameter when saving Transformers pipelines. You also learned how to leverage Custom Pyfunc to add customizations that fit your specific use-case.\n",
    "\n",
    "### What's next?\n",
    "\n",
    "- [In-depth Pyfunc Walkthrough](https://mlflow.org/docs/latest/traditional-ml/creating-custom-pyfunc/index.html). We briefly touched on custom `pyfunc` in this tutorial, but if you're looking for more detail on the anatomy of a `pyfunc` model, the linked page provides an in-depth overview of all the components.\n",
    "- [More on MLflow Deployments](https://mlflow.org/docs/latest/deployment/index.html). In this tutorial, we saw how to deploy a model using a local server, but MLflow provides many other ways to deploy your models to production. Check out this page to learn more about the different options.\n",
    "- [More on MLflow's Transformers Integration](https://mlflow.org/docs/latest/llms/transformers/index.html). This page provides a comprehensive overview on MLflow's Transformers integrations, along with lots of hands-on guides and notebooks. Learn how to fine-tune models, use prompt templates, and more!\n",
    "- [Other LLM Integrations](https://mlflow.org/docs/latest/llms/index.html). Aside from Transformers, MLflow has integrations with many other popular LLM libraries, such as Langchain and OpenAI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlflow-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
