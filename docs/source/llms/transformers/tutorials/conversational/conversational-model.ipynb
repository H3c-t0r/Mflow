{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Conversational AI with MLflow and DialoGPT\n",
    "\n",
    "In this tutorial, we delve into the fascinating world of conversational AI by integrating [Microsoft's DialoGPT](https://huggingface.co/microsoft/DialoGPT-medium), a conversational model, with MLflow's transformers flavor. This guide is designed for those who have a grasp of machine learning workflows and are keen on exploring the realms of natural language processing, specifically in building and managing conversational AI models. We will demonstrate how to utilize MLflow for logging, managing, and deploying a sophisticated chatbot model provided by the [ðŸ¤— Hugging Face](https://huggingface.co/) [Transformers](https://huggingface.co/transformers) library.\n",
    "\n",
    "### What is DialoGPT?\n",
    "\n",
    "DialoGPT is a conversational model developed by Microsoft, fine-tuned on a large dataset of dialogues to generate human-like responses. It's part of the GPT (Generative Pretrained Transformer) family of models, known for their effectiveness in natural language understanding and generation. DialoGPT is specifically optimized for generating conversational responses, making it an ideal choice for building chatbots.\n",
    "\n",
    "### Why MLflow with DialoGPT?\n",
    "\n",
    "Integrating MLflow with DialoGPT offers several benefits:\n",
    "\n",
    "- **Experiment Tracking**: Keep track of various configurations and performance metrics of Conversational models across different experiments.\n",
    "- **Model Management**: Create a centralized repository for different versions of the chatbot models along with their configurations.\n",
    "- **Reproducibility**: Record all necessary components to reproduce the conversational AI model's behavior.\n",
    "- **Deployment**: Streamline the process of deploying Conversational models into production environments.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "In this tutorial, you will:\n",
    "\n",
    "- Set up a conversational AI **pipeline** using DialoGPT from the Transformers library.\n",
    "- **Log** the DialoGPT model along with its configurations using MLflow.\n",
    "- Infer the input and output **signature** of the DialoGPT model.\n",
    "- **Load** a stored DialoGPT model from MLflow for interactive usage.\n",
    "- Interact with the chatbot model and understand the nuances of conversational AI.\n",
    "\n",
    "By the end of this tutorial, you will have a solid understanding of how to manage and deploy conversational AI models with MLflow, enhancing your capabilities in the field of natural language processing.\n",
    "\n",
    "Let's embark on this journey to explore conversational AI with MLflow and DialoGPT!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up the Conversational Pipeline\n",
    "\n",
    "In this section, we begin by importing the necessary libraries: `transformers` and `mlflow`. The `transformers` library, developed by Hugging Face, provides us with a wide range of pre-trained models, including the DialoGPT model for conversational AI. MLflow, on the other hand, is a platform for managing the machine learning lifecycle, including experimentation, reproducibility, and deployment.\n",
    "\n",
    "We then initialize our conversational pipeline using the `transformers.pipeline` function. This function simplifies the process of deploying models for different tasks. Here, we specify the model `\"microsoft/DialoGPT-medium\"`, which is a medium-sized variant of the DialoGPT model, well-suited for general-purpose conversational tasks.\n",
    "\n",
    "After setting up the conversational pipeline, we use MLflow to infer the signature of our model. The signature defines the input and output schema of the model, which is crucial for understanding the data requirements and the format of the model's predictions. We do this by providing a sample input, `\"Hi there, chatbot!\"`, and generating a corresponding output using the `mlflow.transformers.generate_signature_output` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "import mlflow\n",
    "\n",
    "conversational_pipeline = transformers.pipeline(model=\"microsoft/DialoGPT-medium\")\n",
    "\n",
    "signature = mlflow.models.infer_signature(\n",
    "    \"Hi there, chatbot!\",\n",
    "    mlflow.transformers.generate_signature_output(conversational_pipeline, \"Hi there, chatbot!\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the tracking server and creating an experiment\n",
    "\n",
    "In order to view the results in our tracking server (for the purposes of this tutorial, we've started a local tracking server at this url)\n",
    "\n",
    "We can start an instance of the MLflow server locally by running the following from a terminal to start the tracking server:\n",
    "\n",
    "``` bash\n",
    "    mlflow server --host 127.0.0.1 --port 8080\n",
    "```\n",
    "\n",
    "With the server started, the following code will ensure that all experiments, runs, models, parameters, and metrics that we log are being tracked within that server instance (which also provides us with the MLflow UI when navigating to that url address in a browser).\n",
    "\n",
    "After setting the tracking url, we create a new MLflow Experiment to store the run we're about to create in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/664266092508187059', creation_time=1699630163555, experiment_id='664266092508187059', last_update_time=1699630163555, lifecycle_stage='active', name='Conversational', tags={}>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(\"http://127.0.0.1:8080\")\n",
    "\n",
    "mlflow.set_experiment(\"Conversational\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging the Model with MLflow\n",
    "\n",
    "Having set up our conversational pipeline, the next step involves leveraging MLflow for model logging. This process is crucial for versioning, tracking, and managing our model in a more organized manner.\n",
    "\n",
    "We initiate this by starting an MLflow run using `with mlflow.start_run()`. This command begins a new MLflow run, under which all subsequent MLflow commands will log data. Each run is an isolated set of code executions and logs its own parameters, metrics, artifacts, etc.\n",
    "\n",
    "Within this run, we use `mlflow.transformers.log_model` to log our conversational model. This function is specifically tailored for logging transformer models and takes several parameters:\n",
    "\n",
    "- `transformers_model`: Here, we pass our previously created `conversational_pipeline`.\n",
    "- `artifact_path`: This is the location within the MLflow run where our model will be stored. We name it `\"chatbot\"` to reflect its functionality.\n",
    "- `task`: We specify `\"conversational\"` to indicate the nature of our model, which is a required element to construct a pipeline if we save our model as a collection of components and wish to later load it as a constructed `pipeline` object.\n",
    "- `signature`: The model signature we inferred earlier is passed here. It ensures that MLflow knows the expected input and output format of our model.\n",
    "- `input_example`: Providing an example input, such as `\"A clever and witty question\"`, can be helpful for understanding the model's usage and for later testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/Users/benjamin.wilson/miniconda3/envs/mlflow-dev-env/lib/python3.8/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run():\n",
    "    model_info = mlflow.transformers.log_model(\n",
    "        transformers_model=conversational_pipeline,\n",
    "        artifact_path=\"chatbot\",\n",
    "        task=\"conversational\",\n",
    "        signature=signature,\n",
    "        input_example=\"A clever and witty question\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Interacting with the Chatbot Model\n",
    "\n",
    "After successfully logging our conversational model with MLflow, the next step is to load and interact with it. This is where we see the practical application of our model in a conversational context.\n",
    "\n",
    "We begin by loading the model using `mlflow.pyfunc.load_model`. This function is a part of MLflow's Python function (`pyfunc`) flavor, which provides a generic interface for Python models. We pass `model_uri=model_info.model_uri` to specify the location of our logged model. The `model_info.model_uri` contains the URI where our model is stored in MLflow, ensuring that we are loading the exact version of the model we just logged.\n",
    "\n",
    "Once the model is loaded into the variable `chatbot`, we can start interacting with it as if it were a regular Python function. To demonstrate this, we use the `predict` method of our `chatbot` object:\n",
    "\n",
    "- We ask our chatbot, \"What is the best way to get to Antarctica?\" This question is passed as an argument to the `predict` method.\n",
    "- The chatbot's response is then captured and printed. In this case, the response is: \"I think you can get there by boat.\"\n",
    "\n",
    "This interaction showcases the ease with which we can deploy and utilize MLflow-logged models in practical scenarios. The ability to load and use models seamlessly like this is a powerful feature of MLflow, making it an invaluable tool in the machine learning workflow, especially in scenarios requiring quick model deployment and testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "043b2109c835407abe416f5ea24031cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2023/11/10 17:00:44 INFO mlflow.store.artifact.artifact_repo: The progress bar can be disabled by setting the environment variable MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR to false\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffcd0cbe0fb74fe49dee5d92db2c7d16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    }
   ],
   "source": [
    "chatbot = mlflow.pyfunc.load_model(model_uri=model_info.model_uri)\n",
    "\n",
    "first = chatbot.predict(\"What is the best way to get to Antarctica?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: I think you can get there by boat.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Response: {first}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuing the Conversation with the Chatbot\n",
    "\n",
    "In this part of our tutorial, we continue our interaction with the chatbot, further exploring the capabilities of the MLflow `pyfunc` implementation for the `transformers` package. One of the key features of this `pyfunc` implementation for Conversational Pipelines is its ability to maintain conversational context or state. This means that the model remembers the flow of the conversation, allowing for more coherent and contextually relevant responses. This is a feature enabled only for this type of Pipeline. For any other `task` types, if state is required, you will have to manage that yourself with an appropriate prompt and response stateful implementation.\n",
    "\n",
    "We proceed with a follow-up question to our chatbot: \"What sort of boat should I use?\" This question is designed to test the chatbot's ability to maintain the context of the conversation, which started with a question about traveling to Antarctica.\n",
    "\n",
    "The response we receive is: \"A boat that can go to Antarctica.\" This response, while somewhat obvious, demonstrates the model's ability to keep track of the conversation's topic and serves to illustrate the state management present in the MLflow implementation for this pipeline type. \n",
    "\n",
    "It's important to note the witty and somewhat facetious nature of the response. This characteristic is a direct result of the training data used for the DialoGPT model, which includes (heavily sanitized) conversational exchanges from Reddit. Reddit, known for its diverse and often humorous content, influences the style and tone of the responses generated by the model.\n",
    "\n",
    "This interaction highlights the importance of understanding the source and nature of the training data when working with machine learning models. The training data not only determines the model's knowledge base but also its conversational style and tone. In practical applications, this understanding is crucial for setting appropriate expectations and for tailoring the model to suit specific conversational needs or scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    }
   ],
   "source": [
    "second = chatbot.predict(\"What sort of boat should I use?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: A boat that can go to Antarctica.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Response: {second}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Key Takeaways\n",
    "\n",
    "In this tutorial, we've explored the integration of MLflow with a conversational AI model, specifically using the DialoGPT model from Microsoft. We've covered several important aspects and techniques that are crucial for anyone looking to work with advanced machine learning models in a practical, real-world setting.\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **MLflow for Model Management**: We demonstrated how MLflow can be effectively used for managing and deploying machine learning models. The ability to log models, track experiments, and manage different versions of models is invaluable in a machine learning workflow.\n",
    "\n",
    "2. **Conversational AI**: By using the DialoGPT model, we delved into the world of conversational AI, showcasing how to set up and interact with a conversational model. This included understanding the nuances of maintaining conversational context and the impact of training data on the model's responses.\n",
    "\n",
    "3. **Practical Implementation**: Through practical examples, we showed how to log a model in MLflow, infer a model signature, and use the `pyfunc` model flavor for easy deployment and interaction. This hands-on approach is designed to provide you with the skills needed to implement these techniques in your own projects.\n",
    "\n",
    "4. **Understanding Model Responses**: We emphasized the importance of understanding the nature of the model's training data. This understanding is crucial for interpreting the model's responses and for tailoring the model to specific use cases.\n",
    "\n",
    "### Wrapping Up\n",
    "\n",
    "As we conclude this tutorial, we hope that you have gained a deeper understanding of how to integrate MLflow with conversational AI models and the practical considerations involved in deploying these models. The skills and knowledge acquired here are not only applicable to conversational AI but also to a broader range of machine learning applications.\n",
    "\n",
    "Remember, the field of machine learning is vast and constantly evolving. Continuous learning and experimentation are key to staying updated and making the most out of these exciting technologies.\n",
    "\n",
    "Thank you for joining us in this journey through the world of MLflow and conversational AI. We encourage you to take these learnings and apply them to your own unique challenges and projects. Happy coding!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlflow-dev-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
