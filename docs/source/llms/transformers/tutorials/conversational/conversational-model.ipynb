{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Conversational AI with MLflow and DialoGPT\n",
    "\n",
    "Welcome to our tutorial on integrating [Microsoft's DialoGPT](https://huggingface.co/microsoft/DialoGPT-medium) with MLflow's transformers flavor to explore conversational AI.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "In this tutorial, you will:\n",
    "\n",
    "- Set up a conversational AI **pipeline** using DialoGPT from the Transformers library.\n",
    "- **Log** the DialoGPT model along with its configurations using MLflow.\n",
    "- Infer the input and output **signature** of the DialoGPT model.\n",
    "- **Load** a stored DialoGPT model from MLflow for interactive usage.\n",
    "- Interact with the chatbot model and understand the nuances of conversational AI.\n",
    "\n",
    "By the end of this tutorial, you will have a solid understanding of managing and deploying conversational AI models with MLflow, enhancing your capabilities in natural language processing.\n",
    "\n",
    "<details>\n",
    "    <summary style=\"cursor: pointer; display: flex; align-items: center;\">\n",
    "        <span style=\"margin-right: 10px;\">&#x25BA;</span>\n",
    "        <span>Expand to learn more about DialoGPT and the benefits of integrating MLflow with it.</span>\n",
    "    </summary>\n",
    "    <br/>\n",
    "    <div>\n",
    "        <h4>What is DialoGPT?</h4>\n",
    "        <p>DialoGPT is a conversational model developed by Microsoft, fine-tuned on a large dataset of dialogues to generate human-like responses. Part of the GPT family, DialoGPT excels in natural language understanding and generation, making it ideal for chatbots.</p>\n",
    "    </div>\n",
    "    <div>\n",
    "        <h4>Why MLflow with DialoGPT?</h4>\n",
    "        <p>Integrating MLflow with DialoGPT enhances conversational AI model development:</p>\n",
    "        <ul>\n",
    "            <li><strong>Experiment Tracking</strong>: Tracks configurations and metrics across experiments.</li>\n",
    "            <li><strong>Model Management</strong>: Manages different versions and configurations of chatbot models.</li>\n",
    "            <li><strong>Reproducibility</strong>: Ensures the reproducibility of the model's behavior.</li>\n",
    "            <li><strong>Deployment</strong>: Simplifies deploying conversational models in production.</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "</details>\n",
    "</br>\n",
    "Let's begin our exploration of conversational AI with MLflow and DialoGPT!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up the Conversational Pipeline\n",
    "\n",
    "We begin by setting up a conversational pipeline with DialoGPT using `transformers` and managing it with MLflow.\n",
    "\n",
    "<details>\n",
    "    <summary style=\"cursor: pointer; display: flex; align-items: center;\">\n",
    "        <span style=\"margin-right: 10px;\">&#x25BA;</span>\n",
    "        <span>Expand to learn about configuring the DialoGPT pipeline and inferring its model signature with MLflow.</span>\n",
    "    </summary>\n",
    "    <br/>\n",
    "    <div>\n",
    "        <p>We start by importing essential libraries. The <code>transformers</code> library from Hugging Face offers a rich collection of pre-trained models, including DialoGPT, for various NLP tasks. MLflow, a comprehensive tool for the ML lifecycle, aids in experiment tracking, reproducibility, and deployment.</p>\n",
    "    </div>\n",
    "    <div>\n",
    "        <h4>Initializing the Conversational Pipeline</h4>\n",
    "        <p>Using the <code>transformers.pipeline</code> function, we set up a conversational pipeline. We choose the \"<code>microsoft/DialoGPT-medium</code>\" model, balancing performance and resource efficiency, ideal for conversational AI. This step is pivotal for ensuring the model is ready for interaction and integration into various applications.</p>\n",
    "    </div>\n",
    "    <div>\n",
    "        <h4>Inferring the Model Signature with MLflow</h4>\n",
    "        <p>Model signature is key in defining how the model interacts with input data. To infer it, we use a sample input (\"<code>Hi there, chatbot!</code>\") and leverage <code>mlflow.transformers.generate_signature_output</code> to understand the model's input-output schema. This process ensures clarity in the model's data requirements and prediction format, crucial for seamless deployment and usage.</p>\n",
    "    </div>\n",
    "    <div>\n",
    "        <p>This configuration phase sets the stage for a robust conversational AI system, leveraging the strengths of DialoGPT and MLflow for efficient and effective conversational interactions.</p>\n",
    "    </div>\n",
    "</details>\n",
    "<br/>\n",
    "Now, let's proceed with configuring the DialoGPT model for our conversational AI setup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "import mlflow\n",
    "\n",
    "conversational_pipeline = transformers.pipeline(model=\"microsoft/DialoGPT-medium\")\n",
    "\n",
    "signature = mlflow.models.infer_signature(\n",
    "    \"Hi there, chatbot!\",\n",
    "    mlflow.transformers.generate_signature_output(conversational_pipeline, \"Hi there, chatbot!\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the tracking server and creating an experiment\n",
    "\n",
    "In order to view the results in our tracking server (for the purposes of this tutorial, we've started a local tracking server at this url)\n",
    "\n",
    "We can start an instance of the MLflow server locally by running the following from a terminal to start the tracking server:\n",
    "\n",
    "``` bash\n",
    "    mlflow server --host 127.0.0.1 --port 8080\n",
    "```\n",
    "\n",
    "With the server started, the following code will ensure that all experiments, runs, models, parameters, and metrics that we log are being tracked within that server instance (which also provides us with the MLflow UI when navigating to that url address in a browser).\n",
    "\n",
    "After setting the tracking url, we create a new MLflow Experiment to store the run we're about to create in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/664266092508187059', creation_time=1699630163555, experiment_id='664266092508187059', last_update_time=1699630163555, lifecycle_stage='active', name='Conversational', tags={}>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(\"http://127.0.0.1:8080\")\n",
    "\n",
    "mlflow.set_experiment(\"Conversational\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging the Model with MLflow\n",
    "\n",
    "We'll now use MLflow to log our conversational AI model, ensuring systematic versioning, tracking, and management.\n",
    "\n",
    "<details>\n",
    "    <summary style=\"cursor: pointer; display: flex; align-items: center;\">\n",
    "        <span style=\"margin-right: 10px;\">&#x25BA;</span>\n",
    "        <span>Expand for detailed steps on logging the DialoGPT model in MLflow.</span>\n",
    "    </summary>\n",
    "    <br/>\n",
    "    <div>\n",
    "        <h4>Initiating an MLflow Run</h4>\n",
    "        <p>Our first step is to start an MLflow run with <code>mlflow.start_run()</code>. This action initiates a new tracking environment, capturing all model-related data under a unique run ID. It's a crucial step to segregate and organize different modeling experiments.</p>\n",
    "    </div>\n",
    "    <div>\n",
    "        <h4>Logging the Conversational Model</h4>\n",
    "        <p>We log our DialoGPT conversational model using <code>mlflow.transformers.log_model</code>. This specialized function efficiently logs Transformer models and requires several key parameters:</p>\n",
    "        <ul>\n",
    "            <li><strong>transformers_model</strong>: We pass our DialoGPT conversational pipeline.</li>\n",
    "            <li><strong>artifact_path</strong>: The storage location within the MLflow run, aptly named <code>\"chatbot\"</code>.</li>\n",
    "            <li><strong>task</strong>: Set to <code>\"conversational\"</code> to reflect the model's purpose.</li>\n",
    "            <li><strong>signature</strong>: The inferred model signature, dictating expected inputs and outputs.</li>\n",
    "            <li><strong>input_example</strong>: A sample prompt, like <code>\"A clever and witty question\"</code>, to demonstrate expected usage.</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "    <div>\n",
    "        <p>Through this process, MLflow not only tracks our model but also organizes its metadata, facilitating future retrieval, understanding, and deployment.</p>\n",
    "    </div>\n",
    "</details>\n",
    "<br/>\n",
    "Next, we'll showcase how to load and interact with the logged model for a seamless conversational experience.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/Users/benjamin.wilson/miniconda3/envs/mlflow-dev-env/lib/python3.8/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run():\n",
    "    model_info = mlflow.transformers.log_model(\n",
    "        transformers_model=conversational_pipeline,\n",
    "        artifact_path=\"chatbot\",\n",
    "        task=\"conversational\",\n",
    "        signature=signature,\n",
    "        input_example=\"A clever and witty question\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Interacting with the Chatbot Model\n",
    "\n",
    "Next, we'll load the MLflow-logged chatbot model and interact with it to see it in action.\n",
    "\n",
    "<details>\n",
    "    <summary style=\"cursor: pointer; display: flex; align-items: center;\">\n",
    "        <span style=\"margin-right: 10px;\">&#x25BA;</span>\n",
    "        <span>Expand to view the steps for loading and interacting with the DialoGPT model.</span>\n",
    "    </summary>\n",
    "    <br/>\n",
    "    <div>\n",
    "        <h4>Loading the Model with MLflow</h4>\n",
    "        <p>We use <code>mlflow.pyfunc.load_model</code> to load our conversational AI model. This function is a crucial aspect of MLflow's Python function flavor, offering a versatile way to interact with Python models. By specifying <code>model_uri=model_info.model_uri</code>, we precisely target the stored location of our DialoGPT model within MLflow's tracking system.</p>\n",
    "    </div>\n",
    "    <div>\n",
    "        <h4>Interacting with the Chatbot</h4>\n",
    "        <p>Once loaded, the model, referenced as <code>chatbot</code>, is ready for interaction. We demonstrate its conversational capabilities by:</p>\n",
    "        <ul>\n",
    "            <li><strong>Asking Questions</strong>: Posing a question like \"What is the best way to get to Antarctica?\" to the chatbot.</li>\n",
    "            <li><strong>Capturing Responses</strong>: The chatbot's response, generated through the <code>predict</code> method, provides a practical example of its conversational skills. For instance, it might respond with suggestions about reaching Antarctica by boat.</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "    <div>\n",
    "        <p>This demonstration highlights the practicality and convenience of deploying and using models logged with MLflow, especially in dynamic and interactive scenarios like conversational AI.</p>\n",
    "    </div>\n",
    "</details>\n",
    "<br/>\n",
    "We'll now proceed to load our chatbot and test its conversational prowess with a real-world question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "043b2109c835407abe416f5ea24031cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2023/11/10 17:00:44 INFO mlflow.store.artifact.artifact_repo: The progress bar can be disabled by setting the environment variable MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR to false\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffcd0cbe0fb74fe49dee5d92db2c7d16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    }
   ],
   "source": [
    "chatbot = mlflow.pyfunc.load_model(model_uri=model_info.model_uri)\n",
    "\n",
    "first = chatbot.predict(\"What is the best way to get to Antarctica?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: I think you can get there by boat.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Response: {first}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuing the Conversation with the Chatbot\n",
    "\n",
    "We further explore the MLflow `pyfunc` implementation's conversational contextual statefulness with the DialoGPT chatbot model.\n",
    "\n",
    "<details>\n",
    "    <summary style=\"cursor: pointer; display: flex; align-items: center;\">\n",
    "        <span style=\"margin-right: 10px;\">&#x25BA;</span>\n",
    "        <span>Expand to explore more about the chatbot's conversational context and response style.</span>\n",
    "    </summary>\n",
    "    <br/>\n",
    "    <div>\n",
    "        <h4>Testing Contextual Memory</h4>\n",
    "        <p>We pose a follow-up question, \"What sort of boat should I use?\" to test the chatbot's contextual understanding. The response we get, \"A boat that can go to Antarctica,\" while straightforward, showcases the MLflow pyfunc model's ability to retain and utilize conversation history for coherent responses with <code>ConversationalPipeline</code> types of models.</p>\n",
    "    </div>\n",
    "    <div>\n",
    "        <h4>Understanding the Response Style</h4>\n",
    "        <p>The response's style – witty and slightly facetious – reflects the training data's nature, primarily conversational exchanges from Reddit. This training source significantly influences the model's tone and style, leading to responses that can be humorous and diverse.</p>\n",
    "    </div>\n",
    "    <div>\n",
    "        <h4>Implications of Training Data</h4>\n",
    "        <p>This interaction underlines the importance of the training data's source in shaping the model's responses. When deploying such models in real-world applications, it's essential to understand and consider the training data's influence on the model's conversational style and knowledge base.</p>\n",
    "    </div>\n",
    "</details>\n",
    "<br/>\n",
    "Let's see how our chatbot manages the continuity and style of the conversation with this additional query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    }
   ],
   "source": [
    "second = chatbot.predict(\"What sort of boat should I use?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: A boat that can go to Antarctica.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Response: {second}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion and Key Takeaways\n",
    "\n",
    "In this tutorial, we've explored the integration of MLflow with a conversational AI model, specifically using the DialoGPT model from Microsoft. We've covered several important aspects and techniques that are crucial for anyone looking to work with advanced machine learning models in a practical, real-world setting.\n",
    "\n",
    "#### Key Takeaways\n",
    "\n",
    "1. **MLflow for Model Management**: We demonstrated how MLflow can be effectively used for managing and deploying machine learning models. The ability to log models, track experiments, and manage different versions of models is invaluable in a machine learning workflow.\n",
    "\n",
    "2. **Conversational AI**: By using the DialoGPT model, we delved into the world of conversational AI, showcasing how to set up and interact with a conversational model. This included understanding the nuances of maintaining conversational context and the impact of training data on the model's responses.\n",
    "\n",
    "3. **Practical Implementation**: Through practical examples, we showed how to log a model in MLflow, infer a model signature, and use the `pyfunc` model flavor for easy deployment and interaction. This hands-on approach is designed to provide you with the skills needed to implement these techniques in your own projects.\n",
    "\n",
    "4. **Understanding Model Responses**: We emphasized the importance of understanding the nature of the model's training data. This understanding is crucial for interpreting the model's responses and for tailoring the model to specific use cases.\n",
    "\n",
    "5. **Contextual History**: MLflow's `transformers` `pyfunc` implementation for `ConversationalPipelines` maintains a `Conversation` context without the need for managing state yourself. This enables chat bots to be created with minimal effort, since statefulness is maintained for you.\n",
    "\n",
    "### Wrapping Up\n",
    "\n",
    "As we conclude this tutorial, we hope that you have gained a deeper understanding of how to integrate MLflow with conversational AI models and the practical considerations involved in deploying these models. The skills and knowledge acquired here are not only applicable to conversational AI but also to a broader range of machine learning applications.\n",
    "\n",
    "Remember, the field of machine learning is vast and constantly evolving. Continuous learning and experimentation are key to staying updated and making the most out of these exciting technologies.\n",
    "\n",
    "Thank you for joining us in this journey through the world of MLflow and conversational AI. We encourage you to take these learnings and apply them to your own unique challenges and projects. Happy coding!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlflow-dev-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
