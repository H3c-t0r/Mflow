{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Disable a few less-than-useful UserWarnings from setuptools and pydantic\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import openai\n",
    "\n",
    "import mlflow\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types.schema import ColSpec, ParamSchema, ParamSpec, Schema, TensorSpec\n",
    "\n",
    "assert \"OPENAI_API_KEY\" in os.environ, \" OPENAI_API_KEY environment variable must be set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run():\n",
    "    model_info = mlflow.openai.log_model(\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        task=openai.Embedding,\n",
    "        artifact_path=\"model\",\n",
    "        signature=ModelSignature(\n",
    "            inputs=Schema([ColSpec(type=\"string\", name=None)]),\n",
    "            outputs=Schema([TensorSpec(type=np.dtype(\"float64\"), shape=(-1,))]),\n",
    "            params=ParamSchema([ParamSpec(name=\"batch_size\", dtype=\"long\", default=1024)]),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "model = mlflow.pyfunc.load_model(model_info.model_uri)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_space_after_tags(soup, tags):\n",
    "    \"\"\"\n",
    "    Insert a space after each tag specified in the provided BeautifulSoup object.\n",
    "\n",
    "    :param soup: BeautifulSoup object representing the parsed HTML.\n",
    "    :param tags: List of tag names (as strings) after which space should be inserted.\n",
    "    \"\"\"\n",
    "    for tag_name in tags:\n",
    "        for tag in soup.find_all(tag_name):\n",
    "            tag.insert_after(\" \")\n",
    "\n",
    "def extract_text_from_url(url, id):\n",
    "    \"\"\"\n",
    "    Extract and return text content from a specific section of a webpage.\n",
    "\n",
    "    The function targets a div with class 'section' and id 'llms', then extracts\n",
    "    text from <h>, <li>, and <p> tags, excluding <p> tags within <ul> and any \n",
    "    <li> tags that contain <p> with <a> having class 'reference external'. It also\n",
    "    excludes any standalone 'Note' entries.\n",
    "\n",
    "    :param url: URL of the webpage from which to extract text.\n",
    "    :param id: The target id for the div containing the main text content of the page\n",
    "    :return: A string containing the extracted text or an error message.\n",
    "    \"\"\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the div with class 'section' and id 'llms'\n",
    "        target_div = soup.find('div', {'class': 'section', 'id': id})\n",
    "\n",
    "        if target_div:\n",
    "            # Make href and strong tags more readable\n",
    "            insert_space_after_tags(target_div, ['strong', 'a'])\n",
    "\n",
    "            content_tags = []\n",
    "            for tag in target_div.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'p']):\n",
    "                if tag.name == 'li' and tag.find('p') and tag.find('a', class_='reference external'):\n",
    "                    continue  # Skip this specific <li>\n",
    "                content_tags.append(tag)\n",
    "\n",
    "            return '\\n'.join(tag.get_text(separator=' ', strip=True) \n",
    "                             for tag in content_tags \n",
    "                             if (tag.name != 'p' or tag.find_parent('ul') is None) and tag.get_text(strip=True).lower() != \"note\")\n",
    "        else:\n",
    "            return \"Target element not found.\"\n",
    "    else:\n",
    "        return f\"Failed to retrieve content: Status code {response.status_code}\"\n",
    "\n",
    "# Example usage\n",
    "llm_landing_url = \"https://www.mlflow.org/docs/2.8.1/llms/index.html\"\n",
    "llms_landing_page_content = extract_text_from_url(llm_landing_url, \"llms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLMs\n",
      "LLMs, or Large Language Models, have rapidly become a cornerstone in the machine learning domain, offering\n",
      "immense capabilities ranging from natural language understanding to code generation and more.\n",
      "However, harnessing the full potential of LLMs often involves intricate processes, from interfacing with\n",
      "multiple providers to fine-tuning specific models to achieve desired outcomes.\n",
      "Such complexities can easily become a bottleneck for developers and data scientists aiming to integrate LLM\n",
      "capabilities into their applications.\n",
      "MLflow’s Support for LLMs aims to alleviate these challenges by introducing a suite of features and tools designed with the end-user in mind:\n",
      "MLflow AI Gateway\n",
      "Serving as a unified interface, the MLflow AI Gateway simplifies interactions with multiple LLM providers, such as OpenAI , MosaicML , Cohere , Anthropic , PaLM 2 , AWS Bedrock , and AI21 Labs .\n",
      "In addition to supporting the most popular SaaS LLM providers, the AI Gateway provides an integration to MLflow model serving, allowing you to serve your\n",
      "own LLM or a fine-tuned foundation model within your own serving infrastructure.\n",
      "The MLflow AI Gateway is in active development and has been marked as Experimental .\n",
      "APIs may change as this new feature is refined and its functionality is expanded based on feedback.\n",
      "Benefits of the MLflow AI Gateway\n",
      "Unified Endpoint : No more juggling between multiple provider APIs.\n",
      "Simplified Integrations : One-time setup, no repeated complex integrations.\n",
      "Secure Credential Management : Centralized storage prevents scattered API keys. No hardcoding or user-handled keys.\n",
      "Centralized storage prevents scattered API keys.\n",
      "No hardcoding or user-handled keys.\n",
      "Consistent API Experience : Uniform API across all providers. Easy-to-use REST endpoints and Client API.\n",
      "Uniform API across all providers.\n",
      "Easy-to-use REST endpoints and Client API.\n",
      "Seamless Provider Swapping : Swap providers without touching your code. Zero downtime provider, model, or route swapping.\n",
      "Swap providers without touching your code.\n",
      "Zero downtime provider, model, or route swapping.\n",
      "Explore the Native Provider integrations\n",
      "The MLflow AI Gateway supports a large range of foundational models from popular SaaS model vendors, as well as providing a means of self-hosting your\n",
      "own open source model via an integration with MLflow model serving. To learn more about how to get started using the MLflow AI Gateway to simplify the\n",
      "configuration and management of your LLM serving needs, select the provider that you’re interested in below:\n",
      "Getting Started Examples for each Provider\n",
      "If you’re interested in learning about how to set up the MLflow AI Gateway for a specific provider, follow the links below for our up-to-date\n",
      "documentation on GitHub.\n",
      "Each link will take you to a README file that will explain how to set up a route for the provider. In the same directory as\n",
      "the README, you will find a runnable example of how to query the routes that the example creates, providing you with a quick reference for getting started\n",
      "with your favorite provider!\n",
      "The MLflow and Hugging Face TGI providers are for self-hosted LLM serving of either foundation open-source LLM models, fine-tuned open-source\n",
      "LLM models, or your own custom LLM. The example documentation for these providers will show you how to get started with these, using free-to-use open-source\n",
      "models from the Hugging Face Hub .\n",
      "LLM Evaluation\n",
      "Navigating the vast landscape of Large Language Models (LLMs) can be daunting. Determining the right model, prompt, or service that aligns\n",
      "with a project’s needs is no small feat. Traditional machine learning evaluation metrics often fall short when it comes to assessing the\n",
      "nuanced performance of generative models.\n",
      "Enter MLflow LLM Evaluation . This feature is designed to simplify the evaluation process,\n",
      "offering a streamlined approach to compare foundational models, providers, and prompts.\n",
      "Benefits of MLflow’s LLM Evaluation\n",
      "Simplified Evaluation : Navigate the LLM space with ease, ensuring the best fit for your project with standard metrics that can be used to compare generated text.\n",
      "Use-Case Specific Metrics : Leverage MLflow’s mlflow.evaluate() API for a high-level, frictionless evaluation experience.\n",
      "Customizable Metrics : Beyond the provided metrics, MLflow supports a plugin-style for custom scoring, enhancing the evaluation’s flexibility.\n",
      "Comparative Analysis : Effortlessly compare foundational models, providers, and prompts to make informed decisions.\n",
      "Deep Insights : Dive into the intricacies of generative models with a comprehensive suite of LLM-relevant metrics.\n",
      "MLflow’s LLM Evaluation is designed to bridge the gap between traditional machine learning evaluation and the unique challenges posed by LLMs.\n",
      "Prompt Engineering UI\n",
      "Effective utilization of LLMs often hinges on crafting the right prompts.\n",
      "The development of a high-quality prompt is an iterative process of trial and error, where subsequent experimentation is not guaranteed to\n",
      "result in cumulative quality improvements. With the volume and speed of iteration through prompt experimentation, it can quickly become very\n",
      "overwhelming to remember or keep a history of the state of different prompts that were tried.\n",
      "Serving as a powerful tool for prompt engineering, the MLflow Prompt Engineering UI revolutionizes the\n",
      "way developers interact with and refine LLM prompts.\n",
      "Benefits of the MLflow Prompt Engineering UI\n",
      "Iterative Development : Streamlined process for trial and error without the overwhelming complexity.\n",
      "UI-Based Prototyping : Prototype, iterate, and refine prompts without diving deep into code.\n",
      "Accessible Engineering : Makes prompt engineering more user-friendly, speeding up experimentation.\n",
      "Optimized Configurations : Quickly hone in on the best model configurations for tasks like question answering or document summarization.\n",
      "Transparent Tracking : Every model iteration and configuration is meticulously tracked. Ensures reproducibility and transparency in your development process.\n",
      "Every model iteration and configuration is meticulously tracked.\n",
      "Ensures reproducibility and transparency in your development process.\n",
      "The MLflow Prompt Engineering UI is in active development and has been marked as Experimental .\n",
      "Features and interfaces may evolve as feedback is gathered and the tool is refined.\n",
      "Native MLflow Flavors for LLMs\n",
      "Harnessing the power of LLMs becomes effortless with flavors designed specifically for working with LLM libraries and frameworks.\n",
      "Benefits of MLflow’s Native Flavors for LLMs\n",
      "Standardized interfaces for tasks like saving, logging, and managing inference configurations.\n",
      "PyFunc Compatibility : Load models as PyFuncs for broad compatibility across serving infrastructures. Strengthens the MLOps process for LLMs, ensuring smooth deployments.\n",
      "Load models as PyFuncs for broad compatibility across serving infrastructures.\n",
      "Strengthens the MLOps process for LLMs, ensuring smooth deployments.\n",
      "Cohesive Ecosystem : All essential tools and functionalities consolidated under MLflow. Focus on deriving value from LLMs without getting bogged down by interfacing and optimization intricacies.\n",
      "All essential tools and functionalities consolidated under MLflow.\n",
      "Focus on deriving value from LLMs without getting bogged down by interfacing and optimization intricacies.\n",
      "Explore the Native LLM Flavors\n",
      "Select the integration below to read the documentation on how to leverage MLflow’s native integration with these popular libraries:\n",
      "Native Integration Examples\n",
      "If you’d like to directly explore code examples for how to get started with using our official library integrations, you can navigate\n",
      "directly to our up-to-date examples on GitHub below:\n",
      "1 Demonstrates the use of Retrieval Augmented Generation (RAG) using a Vector Store\n",
      "LLM Tracking in MLflow\n",
      "Empowering developers with advanced tracking capabilities, the MLflow LLM Tracking System stands out as the\n",
      "premier solution for managing and analyzing interactions with Large Language Models (LLMs).\n",
      "Benefits of the MLflow LLM Tracking System\n",
      "Robust Interaction Management : Comprehensive tracking of every LLM interaction for maximum insight.\n",
      "Tailor-Made for LLMs : Unique features specifically designed for LLMs. From logging prompts to tracking dynamic data, MLflow has it covered.\n",
      "Unique features specifically designed for LLMs.\n",
      "From logging prompts to tracking dynamic data, MLflow has it covered.\n",
      "Deep Model Insight : Introduces ‘predictions’ as a core entity, alongside the existing artifacts, parameters, and metrics. Gain unparalleled understanding of text-generating model behavior and performance.\n",
      "Introduces ‘predictions’ as a core entity, alongside the existing artifacts, parameters, and metrics.\n",
      "Gain unparalleled understanding of text-generating model behavior and performance.\n",
      "Clarity and Repeatability : Ensures consistent and transparent tracking across all LLM interactions. Facilitates informed decision-making and optimization in LLM deployment and utilization.\n",
      "Ensures consistent and transparent tracking across all LLM interactions.\n",
      "Facilitates informed decision-making and optimization in LLM deployment and utilization.\n",
      "Tutorials and Use Case Guides for LLMs in MLflow\n",
      "Interested in learning how to leverage MLflow for your LLM projects?\n",
      "Look in the tutorials and guides below to learn more about interesting use cases that could help to make your journey into leveraging LLMs a bit easier!\n",
      "Learn how to evaluate LLMs with MLflow.\n",
      "Explore the nuances of packaging, customizing, and deploying advanced LLMs in MLflow using custom PyFuncs.\n",
      "Learn how to leverage LLMs to generate a question dataset for use in Retrieval Augmented Generation applications.\n"
     ]
    }
   ],
   "source": [
    "print(llms_landing_page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-0.02396763674914837,\n",
       "  0.01575297862291336,\n",
       "  0.009518012404441833,\n",
       "  -0.021205933764576912,\n",
       "  -0.009461650624871254,\n",
       "  0.010039353743195534,\n",
       "  -0.008038528263568878,\n",
       "  0.017162011936306953,\n",
       "  -0.02437625639140606,\n",
       "  -0.05867209658026695,\n",
       "  0.0198532622307539,\n",
       "  0.015513443388044834,\n",
       "  -0.012371301651000977,\n",
       "  0.006192696280777454,\n",
       "  -0.015302089042961597,\n",
       "  0.005019676871597767,\n",
       "  0.01812015287578106,\n",
       "  -0.005703057628124952,\n",
       "  0.001988496631383896,\n",
       "  -0.011561108753085136,\n",
       "  0.004537083208560944,\n",
       "  -0.0232208501547575,\n",
       "  -0.015597985126078129,\n",
       "  -0.029082423076033592,\n",
       "  -0.021389108151197433,\n",
       "  0.018261056393384933,\n",
       "  0.025461209937930107,\n",
       "  -0.05021790415048599,\n",
       "  -0.023657649755477905,\n",
       "  0.005896799266338348,\n",
       "  0.020529597997665405,\n",
       "  0.004248231649398804,\n",
       "  -0.023136306554079056,\n",
       "  0.015090733766555786,\n",
       "  0.010285934433341026,\n",
       "  -0.004896386526525021,\n",
       "  0.027236590161919594,\n",
       "  0.005322618875652552,\n",
       "  0.029195144772529602,\n",
       "  -0.0070275478065013885,\n",
       "  0.0318441241979599,\n",
       "  0.023192668333649635,\n",
       "  0.00502319959923625,\n",
       "  -0.01564025692641735,\n",
       "  -0.006365302484482527,\n",
       "  0.005921457428485155,\n",
       "  -0.0017058094963431358,\n",
       "  -0.005956683307886124,\n",
       "  -0.019825082272291183,\n",
       "  0.01314627006649971,\n",
       "  0.008933263830840588,\n",
       "  0.017457908019423485,\n",
       "  -0.00903189554810524,\n",
       "  -0.0024851805064827204,\n",
       "  -0.008954399265348911,\n",
       "  0.007686270400881767,\n",
       "  0.003952335100620985,\n",
       "  0.003045270685106516,\n",
       "  0.016344772651791573,\n",
       "  -0.007263560779392719,\n",
       "  -0.006692902650684118,\n",
       "  0.011131353676319122,\n",
       "  -0.02072686329483986,\n",
       "  0.01062410231679678,\n",
       "  0.010638192296028137,\n",
       "  -0.012624927796423435,\n",
       "  -0.0018493547104299068,\n",
       "  0.0323231965303421,\n",
       "  0.011504746973514557,\n",
       "  -0.024122629314661026,\n",
       "  0.0002600104489829391,\n",
       "  -0.006590747740119696,\n",
       "  0.017246553674340248,\n",
       "  -0.020092798396945,\n",
       "  0.005505792796611786,\n",
       "  -0.014963921159505844,\n",
       "  -0.024982139468193054,\n",
       "  -0.014280540868639946,\n",
       "  -0.018204694613814354,\n",
       "  -0.0039171092212200165,\n",
       "  0.007277650758624077,\n",
       "  0.003011806169524789,\n",
       "  0.012300850823521614,\n",
       "  0.04260913282632828,\n",
       "  0.018218785524368286,\n",
       "  0.008926218375563622,\n",
       "  -0.007376282941550016,\n",
       "  -0.01314627006649971,\n",
       "  0.015090733766555786,\n",
       "  0.006013044621795416,\n",
       "  0.005629083141684532,\n",
       "  -0.011934502050280571,\n",
       "  0.016457494348287582,\n",
       "  0.01649976521730423,\n",
       "  0.0035120125394314528,\n",
       "  -0.004484244622290134,\n",
       "  -0.0016776288393884897,\n",
       "  -0.011356798931956291,\n",
       "  0.011046811938285828,\n",
       "  -0.024277623742818832,\n",
       "  -0.00934892799705267,\n",
       "  0.018740126863121986,\n",
       "  -0.04258095100522041,\n",
       "  -0.004875251092016697,\n",
       "  -0.03919927403330803,\n",
       "  -0.031928665935993195,\n",
       "  0.0011791837168857455,\n",
       "  -0.008559870533645153,\n",
       "  0.01597842387855053,\n",
       "  -0.0017885901033878326,\n",
       "  -0.032858628779649734,\n",
       "  0.019585546106100082,\n",
       "  -0.011166579090058804,\n",
       "  -0.048667971044778824,\n",
       "  -1.1076864211645443e-05,\n",
       "  -0.007651044521480799,\n",
       "  0.00662949588149786,\n",
       "  -0.0030875415541231632,\n",
       "  -0.004533560946583748,\n",
       "  -0.008123070001602173,\n",
       "  0.04111555591225624,\n",
       "  0.02599664218723774,\n",
       "  0.028518810868263245,\n",
       "  -0.01088477298617363,\n",
       "  0.020487327128648758,\n",
       "  -0.0027440900448709726,\n",
       "  0.014808927662670612,\n",
       "  -0.019895533099770546,\n",
       "  -0.027532488107681274,\n",
       "  -0.0011730192927643657,\n",
       "  0.0049774060025811195,\n",
       "  0.003217877121642232,\n",
       "  0.0045546963810920715,\n",
       "  -0.0029219803400337696,\n",
       "  -0.03319679573178291,\n",
       "  0.014019869267940521,\n",
       "  -0.0029748191591352224,\n",
       "  -0.01372397318482399,\n",
       "  -0.010173211805522442,\n",
       "  0.008447147905826569,\n",
       "  0.01893739216029644,\n",
       "  0.03195684775710106,\n",
       "  -0.006002476904541254,\n",
       "  -0.004420838318765163,\n",
       "  -0.026391170918941498,\n",
       "  0.03643757104873657,\n",
       "  0.020008256658911705,\n",
       "  -0.00586157338693738,\n",
       "  0.005664309021085501,\n",
       "  -0.026095274835824966,\n",
       "  -0.00903189554810524,\n",
       "  -0.037198446691036224,\n",
       "  -0.019994165748357773,\n",
       "  0.005153534933924675,\n",
       "  0.021501829847693443,\n",
       "  0.0013764481991529465,\n",
       "  0.011920412071049213,\n",
       "  0.043172743171453476,\n",
       "  -0.03770570084452629,\n",
       "  -0.011666785925626755,\n",
       "  -0.0006499160663224757,\n",
       "  0.010447973385453224,\n",
       "  0.0043362961150705814,\n",
       "  0.008926218375563622,\n",
       "  0.010722734034061432,\n",
       "  0.013526707887649536,\n",
       "  0.02119184285402298,\n",
       "  -0.019261468201875687,\n",
       "  -0.015442991629242897,\n",
       "  0.009525056928396225,\n",
       "  0.0033358836080878973,\n",
       "  0.027236590161919594,\n",
       "  -0.02832154557108879,\n",
       "  0.021332746371626854,\n",
       "  -0.012568566016852856,\n",
       "  -0.011342708952724934,\n",
       "  0.03057599626481533,\n",
       "  0.005604425445199013,\n",
       "  -0.019529184326529503,\n",
       "  -0.021699095144867897,\n",
       "  0.05049971118569374,\n",
       "  -0.006065883208066225,\n",
       "  -0.003284806152805686,\n",
       "  0.015442991629242897,\n",
       "  -0.032971352338790894,\n",
       "  -0.022206345573067665,\n",
       "  0.0007489886484108865,\n",
       "  -0.019754629582166672,\n",
       "  0.0035032061859965324,\n",
       "  -0.011272257193922997,\n",
       "  -0.012688334099948406,\n",
       "  0.03677573800086975,\n",
       "  -0.00028466852381825447,\n",
       "  -0.01325899176299572,\n",
       "  -0.6059965491294861,\n",
       "  -0.01649976521730423,\n",
       "  0.018542863428592682,\n",
       "  -0.013181495480239391,\n",
       "  -0.019021933898329735,\n",
       "  -0.00557272182777524,\n",
       "  -0.013702837750315666,\n",
       "  -0.006826760713011026,\n",
       "  -0.00273704482242465,\n",
       "  0.031421415507793427,\n",
       "  -0.00960959866642952,\n",
       "  0.009285521693527699,\n",
       "  -0.003976993262767792,\n",
       "  -0.006044747773557901,\n",
       "  -0.009496876038610935,\n",
       "  -0.006287805736064911,\n",
       "  -0.021910449489951134,\n",
       "  -0.0381002277135849,\n",
       "  0.006502683274447918,\n",
       "  0.00906007643789053,\n",
       "  -0.0318441241979599,\n",
       "  0.019007842987775803,\n",
       "  -0.01823287643492222,\n",
       "  0.0023495610803365707,\n",
       "  -0.030266009271144867,\n",
       "  0.007693315390497446,\n",
       "  0.023136306554079056,\n",
       "  0.038776565343141556,\n",
       "  0.011018631048500538,\n",
       "  0.02264314517378807,\n",
       "  -0.031139610335230827,\n",
       "  0.0007961031515151262,\n",
       "  -0.005699534900486469,\n",
       "  -0.006171560846269131,\n",
       "  0.051091503351926804,\n",
       "  0.012836282141506672,\n",
       "  -0.02143137902021408,\n",
       "  0.016950655728578568,\n",
       "  0.006407573353499174,\n",
       "  0.018542863428592682,\n",
       "  -0.017908798530697823,\n",
       "  0.011039766483008862,\n",
       "  0.004815367050468922,\n",
       "  0.01007458008825779,\n",
       "  -0.0039805155247449875,\n",
       "  0.028180642053484917,\n",
       "  0.019430553540587425,\n",
       "  0.005259212572127581,\n",
       "  -0.015090733766555786,\n",
       "  -0.00865145679563284,\n",
       "  0.003275999566540122,\n",
       "  0.01742972806096077,\n",
       "  0.0065977927297353745,\n",
       "  0.002136444905772805,\n",
       "  -0.008475327864289284,\n",
       "  0.007791947573423386,\n",
       "  0.02467215247452259,\n",
       "  -0.010786141268908978,\n",
       "  0.01893739216029644,\n",
       "  -0.010757960379123688,\n",
       "  -0.006016567349433899,\n",
       "  -0.002708864165470004,\n",
       "  -0.04441269114613533,\n",
       "  -0.011652695946395397,\n",
       "  -0.010222528129816055,\n",
       "  0.015090733766555786,\n",
       "  0.012603792361915112,\n",
       "  -0.008327379822731018,\n",
       "  0.00021520763402804732,\n",
       "  -0.0133153535425663,\n",
       "  0.012653108686208725,\n",
       "  0.01661248877644539,\n",
       "  0.003998128697276115,\n",
       "  -0.007362192962318659,\n",
       "  0.016175689175724983,\n",
       "  0.013456257060170174,\n",
       "  0.02379855327308178,\n",
       "  0.00239007081836462,\n",
       "  0.00016798304568510503,\n",
       "  0.016429314389824867,\n",
       "  0.02269950695335865,\n",
       "  -0.01730291359126568,\n",
       "  0.0018687288975343108,\n",
       "  -0.017880616709589958,\n",
       "  0.022262707352638245,\n",
       "  0.016316592693328857,\n",
       "  0.010800231248140335,\n",
       "  0.004061535000801086,\n",
       "  0.0027828384190797806,\n",
       "  0.017500178888440132,\n",
       "  0.025700746104121208,\n",
       "  0.032971352338790894,\n",
       "  -0.03150595724582672,\n",
       "  -0.06571725755929947,\n",
       "  0.014266449958086014,\n",
       "  0.013780334033071995,\n",
       "  -0.01742972806096077,\n",
       "  0.01517527550458908,\n",
       "  0.009602554142475128,\n",
       "  -0.02936423011124134,\n",
       "  0.005939070601016283,\n",
       "  -0.028814706951379776,\n",
       "  0.018190603703260422,\n",
       "  0.017739715054631233,\n",
       "  0.021501829847693443,\n",
       "  0.010462063364684582,\n",
       "  -0.006907779723405838,\n",
       "  -0.007989211939275265,\n",
       "  0.00799625739455223,\n",
       "  -0.029927842319011688,\n",
       "  -0.004857638385146856,\n",
       "  -0.008658502250909805,\n",
       "  -0.004970361012965441,\n",
       "  -0.019768720492720604,\n",
       "  0.004836502950638533,\n",
       "  -0.03229501470923424,\n",
       "  0.011053857393562794,\n",
       "  -0.006122244521975517,\n",
       "  0.01520345639437437,\n",
       "  -0.023488566279411316,\n",
       "  0.02003643661737442,\n",
       "  0.0030082836747169495,\n",
       "  0.022629056125879288,\n",
       "  0.008566915057599545,\n",
       "  0.00521341897547245,\n",
       "  -0.002074799733236432,\n",
       "  0.009968901984393597,\n",
       "  -0.011547017842531204,\n",
       "  -0.005526928696781397,\n",
       "  0.01841604895889759,\n",
       "  0.01520345639437437,\n",
       "  -0.01871194690465927,\n",
       "  0.01685202494263649,\n",
       "  -0.027109777554869652,\n",
       "  0.027011144906282425,\n",
       "  0.005378980189561844,\n",
       "  0.0391429103910923,\n",
       "  -0.0022755868267267942,\n",
       "  0.007312876638025045,\n",
       "  0.00393824465572834,\n",
       "  -0.023671738803386688,\n",
       "  0.006696424912661314,\n",
       "  0.01462575327605009,\n",
       "  -0.017035197466611862,\n",
       "  -0.043172743171453476,\n",
       "  -0.03237955644726753,\n",
       "  -0.021558191627264023,\n",
       "  0.010588875971734524,\n",
       "  0.01146247610449791,\n",
       "  -0.027067506685853004,\n",
       "  -0.03201321139931679,\n",
       "  -0.03339406102895737,\n",
       "  -0.022149985656142235,\n",
       "  0.018613314256072044,\n",
       "  -0.0022773481905460358,\n",
       "  -0.005956683307886124,\n",
       "  -0.018190603703260422,\n",
       "  -0.029505131766200066,\n",
       "  -0.013780334033071995,\n",
       "  -0.0396219827234745,\n",
       "  -0.014527121558785439,\n",
       "  0.014470759779214859,\n",
       "  -0.020825494080781937,\n",
       "  0.02286859042942524,\n",
       "  0.005386025179177523,\n",
       "  -0.009722321294248104,\n",
       "  -0.0052697802893817425,\n",
       "  0.016006605699658394,\n",
       "  -0.022558603435754776,\n",
       "  -0.03294317051768303,\n",
       "  -0.014780746772885323,\n",
       "  -0.036099404096603394,\n",
       "  -0.008785314857959747,\n",
       "  0.015555714257061481,\n",
       "  0.011631560511887074,\n",
       "  0.0059038447216153145,\n",
       "  0.0031086772214621305,\n",
       "  0.022375430911779404,\n",
       "  0.0007802515174262226,\n",
       "  -0.008052618242800236,\n",
       "  0.009708231315016747,\n",
       "  0.0312805138528347,\n",
       "  8.123950829030946e-05,\n",
       "  -0.01123703084886074,\n",
       "  0.020304152742028236,\n",
       "  0.01713383011519909,\n",
       "  0.042130060493946075,\n",
       "  0.003346451325342059,\n",
       "  -0.023206759244203568,\n",
       "  0.009884360246360302,\n",
       "  0.011286347173154354,\n",
       "  0.02491168864071369,\n",
       "  -0.01841604895889759,\n",
       "  0.0018211740534752607,\n",
       "  -0.017936978489160538,\n",
       "  0.022079532966017723,\n",
       "  -0.007982166483998299,\n",
       "  -0.015485262498259544,\n",
       "  -0.007256515324115753,\n",
       "  0.029279686510562897,\n",
       "  -0.0028215867932885885,\n",
       "  0.011934502050280571,\n",
       "  0.011927456595003605,\n",
       "  -0.01036343164741993,\n",
       "  -0.007150838151574135,\n",
       "  -0.022826319560408592,\n",
       "  0.0022650191094726324,\n",
       "  -0.024629881605505943,\n",
       "  0.012554476037621498,\n",
       "  0.0076580895110964775,\n",
       "  0.0261093657463789,\n",
       "  -0.007397418841719627,\n",
       "  -0.0037339350674301386,\n",
       "  -0.022375430911779404,\n",
       "  0.00022082174837123603,\n",
       "  0.018556952476501465,\n",
       "  0.011082037352025509,\n",
       "  0.0033446899615228176,\n",
       "  -0.0172042828053236,\n",
       "  0.011251121759414673,\n",
       "  -0.004639238119125366,\n",
       "  0.004586399532854557,\n",
       "  0.010053443722426891,\n",
       "  -0.018162423744797707,\n",
       "  0.02346038445830345,\n",
       "  0.0026102319825440645,\n",
       "  -0.012357211671769619,\n",
       "  0.007897624745965004,\n",
       "  -0.005991909187287092,\n",
       "  -0.01888103038072586,\n",
       "  0.008707818575203419,\n",
       "  0.03421130031347275,\n",
       "  0.025813467800617218,\n",
       "  0.007467870134860277,\n",
       "  0.020416874438524246,\n",
       "  0.014907559379935265,\n",
       "  0.024277623742818832,\n",
       "  -0.010250709019601345,\n",
       "  0.04300365969538689,\n",
       "  -0.033929493278265,\n",
       "  0.0015437707770615816,\n",
       "  0.0007811321993358433,\n",
       "  0.005410683341324329,\n",
       "  -0.002472851425409317,\n",
       "  0.02884288690984249,\n",
       "  -0.01181473396718502,\n",
       "  0.040044691413640976,\n",
       "  0.005854528397321701,\n",
       "  -0.01410441193729639,\n",
       "  -0.00946869608014822,\n",
       "  -0.012568566016852856,\n",
       "  0.02102275937795639,\n",
       "  -0.011215895414352417,\n",
       "  -0.0024763739202171564,\n",
       "  0.01759881153702736,\n",
       "  -0.013456257060170174,\n",
       "  0.00458992226049304,\n",
       "  0.0028163029346615076,\n",
       "  0.019078295677900314,\n",
       "  0.008306244388222694,\n",
       "  -0.011377934366464615,\n",
       "  0.016598397865891457,\n",
       "  0.0014803643571212888,\n",
       "  -0.02785656414926052,\n",
       "  0.02043096534907818,\n",
       "  -0.013745108619332314,\n",
       "  0.011018631048500538,\n",
       "  -0.019134655594825745,\n",
       "  -0.022784048691391945,\n",
       "  0.008735999464988708,\n",
       "  -0.019092384725809097,\n",
       "  -0.009081211872398853,\n",
       "  0.007580592762678862,\n",
       "  -0.041087377816438675,\n",
       "  0.040495581924915314,\n",
       "  0.018049702048301697,\n",
       "  0.009975947439670563,\n",
       "  0.031759582459926605,\n",
       "  0.014125547371804714,\n",
       "  0.0453990139067173,\n",
       "  -0.022445881739258766,\n",
       "  -0.03967834264039993,\n",
       "  0.02299540489912033,\n",
       "  -0.011955637484788895,\n",
       "  -0.018190603703260422,\n",
       "  -0.023812642320990562,\n",
       "  -0.049710653722286224,\n",
       "  0.018373778089880943,\n",
       "  0.004970361012965441,\n",
       "  -0.011547017842531204,\n",
       "  0.022544514387845993,\n",
       "  0.006358257494866848,\n",
       "  -0.01604887656867504,\n",
       "  0.012596746906638145,\n",
       "  -0.014470759779214859,\n",
       "  0.023065855726599693,\n",
       "  0.01934601180255413,\n",
       "  -0.010384567081928253,\n",
       "  -0.005417728330940008,\n",
       "  -0.018317418172955513,\n",
       "  0.006319508887827396,\n",
       "  0.010145030915737152,\n",
       "  -0.028589261695742607,\n",
       "  -0.0140762310475111,\n",
       "  0.03130869194865227,\n",
       "  -0.012892643921077251,\n",
       "  -0.030068745836615562,\n",
       "  -0.011596334166824818,\n",
       "  -0.010574785992503166,\n",
       "  -0.00742559926584363,\n",
       "  -0.013519663363695145,\n",
       "  -0.007179018575698137,\n",
       "  0.016020694747567177,\n",
       "  0.005491702817380428,\n",
       "  0.017471998929977417,\n",
       "  0.006002476904541254,\n",
       "  0.0049139996990561485,\n",
       "  0.010447973385453224,\n",
       "  0.03423948213458061,\n",
       "  0.01662657968699932,\n",
       "  -0.0025626772549003363,\n",
       "  -0.02299540489912033,\n",
       "  -0.002280870685353875,\n",
       "  0.025898009538650513,\n",
       "  0.06098290905356407,\n",
       "  0.013033547438681126,\n",
       "  -0.02182590775191784,\n",
       "  -0.001176541787572205,\n",
       "  -0.0453990139067173,\n",
       "  -0.01847241073846817,\n",
       "  -0.0070240250788629055,\n",
       "  -0.02229088731110096,\n",
       "  0.020881855860352516,\n",
       "  0.027757933363318443,\n",
       "  -0.01065228320658207,\n",
       "  0.01545708253979683,\n",
       "  0.0008625918417237699,\n",
       "  0.0035560447722673416,\n",
       "  0.04134100303053856,\n",
       "  0.015386630780994892,\n",
       "  0.01562616601586342,\n",
       "  -0.027307042852044106,\n",
       "  -0.00865145679563284,\n",
       "  0.009208024479448795,\n",
       "  -0.0015358449891209602,\n",
       "  -0.019191017374396324,\n",
       "  0.0016098192427307367,\n",
       "  0.027645209804177284,\n",
       "  0.01614750735461712,\n",
       "  0.01974054053425789,\n",
       "  0.034070394933223724,\n",
       "  0.017796074971556664,\n",
       "  -0.0174860879778862,\n",
       "  -0.03725481033325195,\n",
       "  0.01158224418759346,\n",
       "  -0.0031192449387162924,\n",
       "  0.009405289776623249,\n",
       "  0.013660565949976444,\n",
       "  -0.008581005968153477,\n",
       "  0.04418724775314331,\n",
       "  0.012181082740426064,\n",
       "  -0.0020589481573551893,\n",
       "  -0.0006419902783818543,\n",
       "  -0.006213831715285778,\n",
       "  0.024221261963248253,\n",
       "  0.017443817108869553,\n",
       "  -0.008496463298797607,\n",
       "  0.00487877381965518,\n",
       "  0.0007120015216059983,\n",
       "  -0.02808200940489769,\n",
       "  -0.0022033739369362593,\n",
       "  0.026898423209786415,\n",
       "  -0.001277816016227007,\n",
       "  -0.007693315390497446,\n",
       "  0.018190603703260422,\n",
       "  -0.0045476509258151054,\n",
       "  -0.03254864364862442,\n",
       "  -0.013251947239041328,\n",
       "  0.03956562280654907,\n",
       "  0.019543275237083435,\n",
       "  0.0109481792896986,\n",
       "  0.010173211805522442,\n",
       "  0.007693315390497446,\n",
       "  -0.0490906797349453,\n",
       "  -0.017570629715919495,\n",
       "  -0.028856977820396423,\n",
       "  -0.007961031049489975,\n",
       "  -0.015442991629242897,\n",
       "  0.0017322289058938622,\n",
       "  0.009327792562544346,\n",
       "  -0.015879791229963303,\n",
       "  0.01871194690465927,\n",
       "  -0.03139323368668556,\n",
       "  0.011878141202032566,\n",
       "  0.0018933869432657957,\n",
       "  -0.032576821744441986,\n",
       "  -0.03531034663319588,\n",
       "  -0.009109392762184143,\n",
       "  0.00946869608014822,\n",
       "  0.011730192229151726,\n",
       "  0.013111043721437454,\n",
       "  0.0004896386526525021,\n",
       "  0.006516773719340563,\n",
       "  0.001215290161781013,\n",
       "  0.0009493354009464383,\n",
       "  0.0023654126562178135,\n",
       "  -0.00408267043530941,\n",
       "  -0.026799790561199188,\n",
       "  -0.007277650758624077,\n",
       "  -0.029392410069704056,\n",
       "  -0.018218785524368286,\n",
       "  -0.017570629715919495,\n",
       "  -0.0167252104729414,\n",
       "  0.0064533669501543045,\n",
       "  -0.01175132766366005,\n",
       "  0.0027687482070177794,\n",
       "  -0.02512304298579693,\n",
       "  0.0050584254786372185,\n",
       "  0.00699232192710042,\n",
       "  0.041425544768571854,\n",
       "  0.023657649755477905,\n",
       "  0.009193934500217438,\n",
       "  -0.009229159913957119,\n",
       "  -0.03480309247970581,\n",
       "  -0.002530973870307207,\n",
       "  -0.03271772712469101,\n",
       "  -0.019374191761016846,\n",
       "  0.003376393113285303,\n",
       "  -0.001955032115802169,\n",
       "  -0.004632193129509687,\n",
       "  0.024939868599176407,\n",
       "  0.020120978355407715,\n",
       "  -0.012427663430571556,\n",
       "  -0.00845419242978096,\n",
       "  -0.007897624745965004,\n",
       "  0.0029801030177623034,\n",
       "  0.015541624277830124,\n",
       "  -0.007686270400881767,\n",
       "  0.00889099296182394,\n",
       "  -0.01117362454533577,\n",
       "  0.02033233270049095,\n",
       "  -9.731128375278786e-05,\n",
       "  0.00781308300793171,\n",
       "  -0.0365784727036953,\n",
       "  -0.009982992894947529,\n",
       "  -0.0349721759557724,\n",
       "  0.04353909194469452,\n",
       "  0.0051183090545237064,\n",
       "  -0.028631532564759254,\n",
       "  0.03843839466571808,\n",
       "  0.01673930138349533,\n",
       "  -0.005777031648904085,\n",
       "  -0.016978837549686432,\n",
       "  0.011159534566104412,\n",
       "  0.005710102617740631,\n",
       "  0.03987560793757439,\n",
       "  0.004984450992196798,\n",
       "  0.004801277071237564,\n",
       "  -0.01888103038072586,\n",
       "  -0.015133004635572433,\n",
       "  -0.040326498448848724,\n",
       "  0.0019585546106100082,\n",
       "  -0.015090733766555786,\n",
       "  -0.004776618909090757,\n",
       "  0.009912541136145592,\n",
       "  0.023826733231544495,\n",
       "  0.009518012404441833,\n",
       "  -0.017415637150406837,\n",
       "  0.008334425278007984,\n",
       "  -0.030153287574648857,\n",
       "  0.0008832869934849441,\n",
       "  0.029082423076033592,\n",
       "  0.0034415610134601593,\n",
       "  0.02229088731110096,\n",
       "  -0.03195684775710106,\n",
       "  0.0031949803233146667,\n",
       "  -0.05664309114217758,\n",
       "  -0.040918294340372086,\n",
       "  -0.006122244521975517,\n",
       "  -0.00506899319589138,\n",
       "  0.00038374107680283487,\n",
       "  -0.007460825145244598,\n",
       "  0.0406646654009819,\n",
       "  0.007918760180473328,\n",
       "  0.04590626806020737,\n",
       "  -0.003045270685106516,\n",
       "  0.01597842387855053,\n",
       "  0.0029941932298243046,\n",
       "  -0.005667831748723984,\n",
       "  -0.028828797861933708,\n",
       "  0.02668706886470318,\n",
       "  -0.007975121960043907,\n",
       "  0.0021593417041003704,\n",
       "  -0.002597902901470661,\n",
       "  0.007066295947879553,\n",
       "  0.016175689175724983,\n",
       "  0.005393070634454489,\n",
       "  -0.00011800644278991967,\n",
       "  0.009461650624871254,\n",
       "  0.015738889575004578,\n",
       "  5.949858314124867e-05,\n",
       "  -0.009982992894947529,\n",
       "  -0.01882466860115528,\n",
       "  -0.029956022277474403,\n",
       "  -0.013597159646451473,\n",
       "  -0.0027388061862438917,\n",
       "  0.009489831514656544,\n",
       "  0.0025908579118549824,\n",
       "  -0.044609956443309784,\n",
       "  0.0032495802734047174,\n",
       "  0.009982992894947529,\n",
       "  -0.0004958031931892037,\n",
       "  0.013526707887649536,\n",
       "  0.01196268294006586,\n",
       "  0.027546577155590057,\n",
       "  -0.013547844253480434,\n",
       "  0.01465393416583538,\n",
       "  0.021670913323760033,\n",
       "  0.014231224544346333,\n",
       "  -0.022037262097001076,\n",
       "  -0.010433882474899292,\n",
       "  -0.034887634217739105,\n",
       "  -0.01719019189476967,\n",
       "  0.028448358178138733,\n",
       "  0.0014380933716893196,\n",
       "  0.00975754763931036,\n",
       "  0.007341057527810335,\n",
       "  -0.021966811269521713,\n",
       "  0.005410683341324329,\n",
       "  -0.010391611605882645,\n",
       "  -0.013914192095398903,\n",
       "  -0.008116024546325207,\n",
       "  -0.006689379923045635,\n",
       "  -0.0355357900261879,\n",
       "  -0.0034891157411038876,\n",
       "  -0.028800616040825844,\n",
       "  -0.016950655728578568,\n",
       "  -0.022206345573067665,\n",
       "  0.002286154543980956,\n",
       "  0.006900734733790159,\n",
       "  0.025292126461863518,\n",
       "  0.01161042507737875,\n",
       "  -0.02264314517378807,\n",
       "  -0.00561851542443037,\n",
       "  0.037198446691036224,\n",
       "  0.004019264131784439,\n",
       "  0.0422709621489048,\n",
       "  0.006146902684122324,\n",
       "  0.029195144772529602,\n",
       "  -0.0003242975508328527,\n",
       "  0.0016925998497754335,\n",
       "  -0.028406087309122086,\n",
       "  0.00035864271922037005,\n",
       "  -0.008820541203022003,\n",
       "  -0.007094476837664843,\n",
       "  0.01614750735461712,\n",
       "  0.017471998929977417,\n",
       "  -0.0032583868596702814,\n",
       "  -0.00742559926584363,\n",
       "  0.019712358713150024,\n",
       "  -0.001022428972646594,\n",
       "  -0.017049288377165794,\n",
       "  -0.05010518059134483,\n",
       "  0.0232208501547575,\n",
       "  0.011152489110827446,\n",
       "  -0.00393824465572834,\n",
       "  -0.022854501381516457,\n",
       "  -0.02038869448006153,\n",
       "  -0.008186476305127144,\n",
       "  0.01871194690465927,\n",
       "  -0.0005882708937861025,\n",
       "  0.018035611137747765,\n",
       "  0.014111456461250782,\n",
       "  -0.011997908353805542,\n",
       "  0.009045986458659172,\n",
       "  0.039002008736133575,\n",
       "  0.024066269397735596,\n",
       "  0.005706579890102148,\n",
       "  -0.014583482407033443,\n",
       "  -0.00432572839781642,\n",
       "  0.02508077211678028,\n",
       "  -0.005438863765448332,\n",
       "  -0.038184769451618195,\n",
       "  0.005826347973197699,\n",
       "  0.0030276577454060316,\n",
       "  0.03209775313735008,\n",
       "  -0.004237663932144642,\n",
       "  0.017443817108869553,\n",
       "  0.017007017508149147,\n",
       "  0.005012631881982088,\n",
       "  0.0013984644319862127,\n",
       "  -0.005297960713505745,\n",
       "  0.000712882203515619,\n",
       "  0.03139323368668556,\n",
       "  -0.0297587588429451,\n",
       "  0.011074992828071117,\n",
       "  0.00828510895371437,\n",
       "  -0.011469521559774876,\n",
       "  0.007369237951934338,\n",
       "  -0.025912100449204445,\n",
       "  -0.005766463931649923,\n",
       "  0.008644412271678448,\n",
       "  0.02043096534907818,\n",
       "  0.0032249223440885544,\n",
       "  0.021628642454743385,\n",
       "  -0.0010919999331235886,\n",
       "  -0.020628230646252632,\n",
       "  0.008341469801962376,\n",
       "  -0.0017833062447607517,\n",
       "  -0.03260500356554985,\n",
       "  -0.0031333351507782936,\n",
       "  -0.02107912115752697,\n",
       "  0.019360100850462914,\n",
       "  -0.028800616040825844,\n",
       "  0.0025521093048155308,\n",
       "  0.01685202494263649,\n",
       "  0.0040474445559084415,\n",
       "  0.012582656927406788,\n",
       "  0.0037832511588931084,\n",
       "  -0.017049288377165794,\n",
       "  -0.01424531452357769,\n",
       "  0.023770371451973915,\n",
       "  -0.03612758219242096,\n",
       "  0.0086021414026618,\n",
       "  0.012336076237261295,\n",
       "  0.008785314857959747,\n",
       "  -0.006728128530085087,\n",
       "  -0.009144618175923824,\n",
       "  -0.02033233270049095,\n",
       "  -0.015133004635572433,\n",
       "  0.005393070634454489,\n",
       "  -0.0048294574953615665,\n",
       "  -0.03178776428103447,\n",
       "  0.001036519301123917,\n",
       "  -0.0031544705852866173,\n",
       "  -0.0003921072056982666,\n",
       "  0.009461650624871254,\n",
       "  -0.019881444051861763,\n",
       "  0.017570629715919495,\n",
       "  0.00771445082500577,\n",
       "  -0.008031482808291912,\n",
       "  -0.003529625479131937,\n",
       "  0.00473434804007411,\n",
       "  0.022882681339979172,\n",
       "  -0.02409444935619831,\n",
       "  -0.010779095813632011,\n",
       "  0.0001984753762371838,\n",
       "  -0.01730291359126568,\n",
       "  0.011089082807302475,\n",
       "  -0.0002490023907739669,\n",
       "  0.009722321294248104,\n",
       "  -0.021966811269521713,\n",
       "  -0.024700332432985306,\n",
       "  0.0002434983616694808,\n",
       "  -0.00790467020124197,\n",
       "  -0.003582464065402746,\n",
       "  -0.011680875904858112,\n",
       "  0.011032721027731895,\n",
       "  -0.016020694747567177,\n",
       "  -0.023601287975907326,\n",
       "  -0.012004953809082508,\n",
       "  0.007982166483998299,\n",
       "  -0.018246965482831,\n",
       "  0.006217354442924261,\n",
       "  0.006185650825500488,\n",
       "  0.00944051519036293,\n",
       "  0.02264314517378807,\n",
       "  0.008017392829060555,\n",
       "  -0.02958967536687851,\n",
       "  -0.019994165748357773,\n",
       "  -0.005632605869323015,\n",
       "  -0.008926218375563622,\n",
       "  -0.028631532564759254,\n",
       "  -0.02055777795612812,\n",
       "  -0.013230811804533005,\n",
       "  0.050753336399793625,\n",
       "  -0.0022403611801564693,\n",
       "  -0.025855738669633865,\n",
       "  -0.012075405567884445,\n",
       "  -0.01027888897806406,\n",
       "  -0.0417073518037796,\n",
       "  -0.016640668734908104,\n",
       "  -0.005301483441144228,\n",
       "  0.006759831681847572,\n",
       "  -0.007622863631695509,\n",
       "  0.02564438432455063,\n",
       "  0.00699232192710042,\n",
       "  0.013251947239041328,\n",
       "  -0.009630734100937843,\n",
       "  0.01969826966524124,\n",
       "  -0.013047637417912483,\n",
       "  -0.013716927729547024,\n",
       "  0.0035454770550131798,\n",
       "  -0.023883095011115074,\n",
       "  0.025179404765367508,\n",
       "  -0.003342928597703576,\n",
       "  -0.024122629314661026,\n",
       "  0.002964251209050417,\n",
       "  0.004501857794821262,\n",
       "  -0.028222912922501564,\n",
       "  0.001791232032701373,\n",
       "  0.03167504072189331,\n",
       "  -0.022093623876571655,\n",
       "  -0.013209676370024681,\n",
       "  0.012293805368244648,\n",
       "  0.002958967350423336,\n",
       "  -0.00109992572106421,\n",
       "  0.005026721861213446,\n",
       "  0.005033767316490412,\n",
       "  0.015485262498259544,\n",
       "  -0.012223353609442711,\n",
       "  0.009694141335785389,\n",
       "  -0.017500178888440132,\n",
       "  -0.0001657814282225445,\n",
       "  -0.0038677931297570467,\n",
       "  0.016922475770115852,\n",
       "  0.00851055420935154,\n",
       "  -0.003209070535376668,\n",
       "  0.009144618175923824,\n",
       "  -0.011927456595003605,\n",
       "  0.00883463118225336,\n",
       "  -0.014160772785544395,\n",
       "  0.04401816427707672,\n",
       "  0.01204017922282219,\n",
       "  0.004730825312435627,\n",
       "  0.03221047297120094,\n",
       "  -0.004413793329149485,\n",
       "  0.01621796004474163,\n",
       "  0.005153534933924675,\n",
       "  0.0035895092878490686,\n",
       "  0.020177340134978294,\n",
       "  -0.004593444522470236,\n",
       "  0.0355357900261879,\n",
       "  -0.0027934061363339424,\n",
       "  -0.028420178219676018,\n",
       "  -0.008813495747745037,\n",
       "  0.011857004836201668,\n",
       "  0.008686683140695095,\n",
       "  -0.025348488241434097,\n",
       "  -0.005671354476362467,\n",
       "  -0.0024534771218895912,\n",
       "  -0.0407492071390152,\n",
       "  0.006985276937484741,\n",
       "  -0.0023759803734719753,\n",
       "  -0.013759198598563671,\n",
       "  0.0004944822285324335,\n",
       "  -0.004579354543238878,\n",
       "  0.0005732999416068196,\n",
       "  0.000763519259635359,\n",
       "  -0.015471172519028187,\n",
       "  0.022192256525158882,\n",
       "  -0.0014583482407033443,\n",
       "  -0.0006411095964722335,\n",
       "  -0.00267892237752676,\n",
       "  -0.002178716007620096,\n",
       "  -0.006904257461428642,\n",
       "  0.010919999331235886,\n",
       "  0.0297587588429451,\n",
       "  -0.0012839805567637086,\n",
       "  -0.009891405701637268,\n",
       "  0.19253015518188477,\n",
       "  -0.010447973385453224,\n",
       "  -0.0016917191678658128,\n",
       "  0.022953134030103683,\n",
       "  -0.007636954076588154,\n",
       "  0.00444549648091197,\n",
       "  0.023544926196336746,\n",
       "  0.015344359911978245,\n",
       "  -0.0063265543431043625,\n",
       "  0.018726035952568054,\n",
       "  0.019585546106100082,\n",
       "  0.0035648513585329056,\n",
       "  -0.02720841020345688,\n",
       "  0.0019057159079238772,\n",
       "  0.014041004702448845,\n",
       "  -0.03607122227549553,\n",
       "  -0.026475712656974792,\n",
       "  -0.03725481033325195,\n",
       "  -0.027518397197127342,\n",
       "  0.005368412472307682,\n",
       "  -0.0032143546268343925,\n",
       "  0.007439689710736275,\n",
       "  -0.00627019302919507,\n",
       "  -0.0397065244615078,\n",
       "  0.012519250623881817,\n",
       "  -0.007214244455099106,\n",
       "  0.020191431045532227,\n",
       "  0.018500592559576035,\n",
       "  -0.0016556127229705453,\n",
       "  0.014837108552455902,\n",
       "  -0.011941547505557537,\n",
       "  0.017415637150406837,\n",
       "  0.0012223353842273355,\n",
       "  0.00784830842167139,\n",
       "  0.0019374191761016846,\n",
       "  -0.016978837549686432,\n",
       "  0.015597985126078129,\n",
       "  -0.016358863562345505,\n",
       "  0.016767481341958046,\n",
       "  0.006918347906321287,\n",
       "  0.0014160772552713752,\n",
       "  0.012949004769325256,\n",
       "  0.01864149421453476,\n",
       "  -0.03097052499651909,\n",
       "  0.01210358552634716,\n",
       "  0.03215411305427551,\n",
       "  ...]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llms_landing_embeddings = model.predict(llms_landing_page_content)\n",
    "\n",
    "llms_landing_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow LLM Evaluate\n",
      "With the emerging of ChatGPT, LLMs have shown its power of text generation in various fields, such as\n",
      "question answering, translating and text summarization. Evaluating LLMs’ performance is slightly different\n",
      "from traditional ML models, as very often there is no single ground truth to compare against.\n",
      "MLflow provides an API mlflow.evaluate() to help evaluate your LLMs.\n",
      "MLflow’s LLM evaluation functionality consists of 3 main components:\n",
      "A model to evaluate : it can be an MLflow pyfunc model, a URI pointing to one registered\n",
      "MLflow model, or any python callable that represents your model, e.g, a HuggingFace text summarization pipeline.\n",
      "A model to evaluate : it can be an MLflow pyfunc model, a URI pointing to one registered\n",
      "MLflow model, or any python callable that represents your model, e.g, a HuggingFace text summarization pipeline.\n",
      "Metrics : the metrics to compute, LLM evaluate will use LLM metrics.\n",
      "Metrics : the metrics to compute, LLM evaluate will use LLM metrics.\n",
      "Evaluation data : the data your model is evaluated at, it can be a pandas Dataframe, a python list, a\n",
      "numpy array or an mlflow.data.dataset.Dataset() instance.\n",
      "Evaluation data : the data your model is evaluated at, it can be a pandas Dataframe, a python list, a\n",
      "numpy array or an mlflow.data.dataset.Dataset() instance.\n",
      "Full Notebook Guides and Examples\n",
      "If you’re interested in thorough use-case oriented guides that showcase the simplicity and power of MLflow’s evaluate\n",
      "functionality for LLMs, please navigate to the notebook collection below:\n",
      "Quickstart\n",
      "Below is a simple example that gives an quick overview of how MLflow LLM evaluation works. The example builds\n",
      "a simple question-answering model by wrapping “openai/gpt-4” with custom prompt. You can paste it to\n",
      "your IPython or local editor and execute it, and install missing dependencies as prompted. Running the code\n",
      "requires OpenAI API key, if you don’t have an OpenAI key, you can set it up [here]( https://platform.openai.com/account/api-keys ).\n",
      "LLM Evaluation Metrics\n",
      "There are two types of LLM evaluation metrics in MLflow:\n",
      "Metrics relying on SaaS model (e.g., OpenAI) for scoring, e.g., mlflow.metrics.genai.answer_relevance() . These\n",
      "metrics are created via mlflow.metrics.genai.make_genai_metric() method. For each data record, these metrics under the hood sends\n",
      "one prompt consisting of the following information to the SaaS model, and extract the score from model response: Metrics definition. Metrics grading criteria. Reference examples. Input data/context. Model output. [optional] Ground truth. More details of how these fields are set can be found in the section “Create your Custom LLM-evaluation Metrics”.\n",
      "Metrics relying on SaaS model (e.g., OpenAI) for scoring, e.g., mlflow.metrics.genai.answer_relevance() . These\n",
      "metrics are created via mlflow.metrics.genai.make_genai_metric() method. For each data record, these metrics under the hood sends\n",
      "one prompt consisting of the following information to the SaaS model, and extract the score from model response:\n",
      "Metrics definition.\n",
      "Metrics grading criteria.\n",
      "Reference examples.\n",
      "Input data/context.\n",
      "Model output.\n",
      "[optional] Ground truth.\n",
      "More details of how these fields are set can be found in the section “Create your Custom LLM-evaluation Metrics”.\n",
      "Function-based per-row metrics. These metrics calculate a score for each data record (row in terms of Pandas/Spark dataframe),\n",
      "based on certain functions, like Rouge ( mlflow.metrics.rougeL() ) or Flesch Kincaid ( mlflow.metrics.flesch_kincaid_grade_level() ).\n",
      "These metrics are similar to traditional metrics.\n",
      "Function-based per-row metrics. These metrics calculate a score for each data record (row in terms of Pandas/Spark dataframe),\n",
      "based on certain functions, like Rouge ( mlflow.metrics.rougeL() ) or Flesch Kincaid ( mlflow.metrics.flesch_kincaid_grade_level() ).\n",
      "These metrics are similar to traditional metrics.\n",
      "Select Metrics to Evaluate\n",
      "There are two ways to select metrics to evaluate your model:\n",
      "Use default metrics for pre-defined model types.\n",
      "Use a custom list of metrics.\n",
      "Use Default Metrics for Pre-defined Model Types\n",
      "MLflow LLM evaluation includes default collections of metrics for pre-selected tasks, e.g, “question-answering”. Depending on the\n",
      "LLM use case that you are evaluating, these pre-defined collections can greatly simplify the process of running evaluations. To use\n",
      "defaults metrics for pre-selected tasks, specify the model_type argument in mlflow.evaluate() , as shown by the example\n",
      "below:\n",
      "The supported LLM model types and associated metrics are listed below:\n",
      "exact-match\n",
      "1 Requires package evaluate , torch , and transformers\n",
      "2 Requires package textstat\n",
      "3 Requires package evaluate , nltk , and rouge-score\n",
      "Use a Custom List of Metrics\n",
      "Using the pre-defined metrics associated with a given model type is not the only way to generate scoring metrics\n",
      "for LLM evaluation in MLFlow. You can specify a custom list of metrics in the extra_metrics argument in mlflow.evaluate :\n",
      "To add additional metrics to the default metrics list of pre-defined model type, keep the model_type and add your metrics to extra_metrics : results = mlflow . evaluate ( model , eval_data , targets = \"ground_truth\" , model_type = \"question-answering\" , extra_metrics = [ mlflow . metrics . latency ()], ) The above code will evaluate your model using all metrics for “question-answering” model plus mlflow.metrics.latency() .\n",
      "To disable default metric calculation and only calculate your selected metrics, remove the model_type argument and define the desired metrics. results = mlflow . evaluate ( model , eval_data , targets = \"ground_truth\" , extra_metrics = [ mlflow . metrics . toxicity (), mlflow . metrics . latency ()], )\n",
      "The full reference for supported evaluation metrics can be found here .\n",
      "Metrics with LLM as the Judge\n",
      "MLflow offers a few pre-canned metrics which uses LLM as the judge. Despite the difference under the hood, the usage\n",
      "is the same - put these metrics in the extra_metrics argument in mlflow.evaluate() . Here is the list of pre-canned\n",
      "metrics:\n",
      "mlflow.metrics.genai.answer_similarity() : Use this metric when you want to evaluate how similar the model generated output is compared to the information in the ground_truth. High scores mean that your model outputs contain similar information as the ground_truth, while low scores mean that outputs may disagree with the ground_truth.\n",
      "mlflow.metrics.genai.answer_correctness() : Use this metric when you want to evaluate how factually correct the model generated output is based on the information in the ground_truth. High scores mean that your model outputs contain similar information as the ground_truth and that this information is correct, while low scores mean that outputs may disagree with the ground_truth or that the information in the output is incorrect. Note that this builds onto answer_similarity.\n",
      "mlflow.metrics.genai.answer_relevance() : Use this metric when you want to evaluate how relevant the model generated output is to the input (context is ignored). High scores mean that your model outputs are about the same subject as the input, while low scores mean that outputs may be non-topical.\n",
      "mlflow.metrics.genai.relevance() : Use this metric when you want to evaluate how relevant the model generated output is with respect to both the input and the context. High scores mean that the model has understood the context and correct extracted relevant information from the context, while low score mean that output has completely ignored the question and the context and could be hallucinating.\n",
      "mlflow.metrics.genai.faithfulness() : Use this metric when you want to evaluate how faithful the model generated output is based on the context provided. High scores mean that the outputs contain information that is in line with the context, while low scores mean that outputs may disagree with the context (input is ignored).\n",
      "Creating Custom LLM-evaluation Metrics\n",
      "Create LLM-as-judge Evaluation Metrics (Category 1)\n",
      "You can also create your own Saas LLM evaluation metrics with MLflow API mlflow.metrics.genai.make_genai_metric() , which\n",
      "needs the following information:\n",
      "name : the name of your custom metric.\n",
      "definition : describe what’s the metric doing.\n",
      "grading_prompt : describe the scoring critieria.\n",
      "examples : a few input/output examples with score, they are used as a reference for LLM judge.\n",
      "model : the identifier of LLM judge.\n",
      "parameters : the extra parameters to send to LLM judge, e.g., temperature for \"openai:/gpt-3.5-turbo-16k\" .\n",
      "aggregations : The list of options to aggregate the per-row scores using numpy functions.\n",
      "greater_is_better : indicates if a higher score means your model is better.\n",
      "Under the hood, definition , grading_prompt , examples together with evaluation data and model output will be\n",
      "composed into a long prompt and sent to LLM. If you are familiar with the concept of prompt engineering,\n",
      "SaaS LLM evaluation metric is basically trying to compose a “right” prompt containing instructions, data and model\n",
      "output so that LLM, e.g., GPT4 can output the information we want.\n",
      "Now let’s create a custom GenAI metrics called “professionalism”, which measures how professional our model output is.\n",
      "Let’s first create a few examples with scores, these will be the reference samples LLM judge uses. To create such examples,\n",
      "we will use mlflow.metrics.genai.EvaluationExample() class, which has 4 fields:\n",
      "input: input text.\n",
      "output: output text.\n",
      "score: the score for output in the context of input.\n",
      "justification: why do we give the score for the data.\n",
      "Now let’s define the professionalism metric, you will see how each field is set up.\n",
      "Create heuristic-based LLM Evaluation Metrics (Category 2)\n",
      "This is very similar to creating a custom traditional metrics, with the exception of returning a EvaluationResult instance.\n",
      "Basically you need to:\n",
      "Implement a eval_fn to define your scoring logic, it must take in 3 args predictions , targets and metrics . eval_fn must return a mlflow.metrics.MetricValue() instance.\n",
      "Implement a eval_fn to define your scoring logic, it must take in 3 args predictions , targets and metrics . eval_fn must return a mlflow.metrics.MetricValue() instance.\n",
      "Pass eval_fn and other arguments to mlflow.metrics.make_metric API to create the metric.\n",
      "Pass eval_fn and other arguments to mlflow.metrics.make_metric API to create the metric.\n",
      "The following code creates a dummy per-row metric called \"over_10_chars\" : if the model output is greater than 10,\n",
      "the score is 1 otherwise 0.\n",
      "Prepare Your LLM for Evaluating\n",
      "In order to evaluate your LLM with mlflow.evaluate() , your LLM has to be one of the following type:\n",
      "A mlflow.pyfunc.PyFuncModel() instance or a URI pointing to a logged mlflow.pyfunc.PyFuncModel model. In\n",
      "general we call that MLflow model. The\n",
      "A mlflow.pyfunc.PyFuncModel() instance or a URI pointing to a logged mlflow.pyfunc.PyFuncModel model. In\n",
      "general we call that MLflow model. The\n",
      "A python function that takes in string inputs and outputs a single string. Your callable must match the signature of mlflow.pyfunc.PyFuncModel.predict() (without params argument), briefly it should: Has data as the only argument, which can be a pandas.Dataframe , numpy.ndarray , python list, dictionary or scipy matrix. Returns one of pandas.DataFrame , pandas.Series , numpy.ndarray or list.\n",
      "A python function that takes in string inputs and outputs a single string. Your callable must match the signature of mlflow.pyfunc.PyFuncModel.predict() (without params argument), briefly it should:\n",
      "Has data as the only argument, which can be a pandas.Dataframe , numpy.ndarray , python list, dictionary or scipy matrix.\n",
      "Returns one of pandas.DataFrame , pandas.Series , numpy.ndarray or list.\n",
      "Set model=None , and put model outputs in data . Only applicable when the data is a Pandas dataframe.\n",
      "Set model=None , and put model outputs in data . Only applicable when the data is a Pandas dataframe.\n",
      "Evaluating with an MLflow Model\n",
      "For detailed instruction on how to convert your model into a mlflow.pyfunc.PyFuncModel instance, please read this doc . But in short,\n",
      "to evaluate your model as an MLflow model, we recommend following the steps below:\n",
      "Package your LLM as an MLflow model and log it to MLflow server by log_model . Each flavor ( opeanai , pytorch , …)\n",
      "has its own log_model API, e.g., mlflow.openai.log_model() : with mlflow . start_run (): system_prompt = \"Answer the following question in two sentences\" # Wrap \"gpt-3.5-turbo\" as an MLflow model. logged_model_info = mlflow . openai . log_model ( model = \"gpt-3.5-turbo\" , task = openai . ChatCompletion , artifact_path = \"model\" , messages = [ { \"role\" : \"system\" , \"content\" : system_prompt }, { \"role\" : \"user\" , \"content\" : \" {question} \" }, ], )\n",
      "Package your LLM as an MLflow model and log it to MLflow server by log_model . Each flavor ( opeanai , pytorch , …)\n",
      "has its own log_model API, e.g., mlflow.openai.log_model() :\n",
      "Use the URI of logged model as the model instance in mlflow.evaluate() : results = mlflow . evaluate ( logged_model_info . model_uri , eval_data , targets = \"ground_truth\" , model_type = \"question-answering\" , )\n",
      "Use the URI of logged model as the model instance in mlflow.evaluate() :\n",
      "Evaluating with a Custom Function\n",
      "As of MLflow 2.8.0, mlflow.evaluate() supports evaluating a python function without requiring\n",
      "logging the model to MLflow. This is useful when you don’t want to log the model and just want to evaluate\n",
      "it. The following example uses mlflow.evaluate() to evaluate a function. You also need to set\n",
      "up OpenAI authentication to run the code below.\n",
      "Evaluating with a Static Dataset\n",
      "For MLflow >= 2.8.0, mlflow.evaluate() supports evaluating a static dataset without specifying a model.\n",
      "This is useful when you save the model output to a column in a Pandas DataFrame or an MLflow PandasDataset, and\n",
      "want to evaluate the static dataset without re-running the model.\n",
      "If you are using a Pandas DataFrame, you must specify the column name that contains the model output using the\n",
      "top-level predictions parameter in mlflow.evaluate() :\n",
      "Viewing Evaluation Results\n",
      "View Evaluation Results via Code\n",
      "mlflow.evaluate() returns the evaluation results as an mlflow.models.EvaluationResult() instace.\n",
      "To see the score on selected metrics, you can check:\n",
      "metrics : stores the aggregated results, like average/variance across the evaluation dataset. Let’s take a second\n",
      "pass on the code example above and focus on printing out the aggregated results. with mlflow . start_run () as run : results = mlflow . evaluate ( data = eval_data , targets = \"ground_truth\" , predictions = \"predictions\" , extra_metrics = [ mlflow . metrics . genai . answer_similarity ()], evaluators = \"default\" , ) print ( f \"See aggregated evaluation results below: \\n { results . metrics } \" )\n",
      "tables[\"eval_results_table\"] : stores the per-row evaluation results. with mlflow . start_run () as run : results = mlflow . evaluate ( data = eval_data , targets = \"ground_truth\" , predictions = \"predictions\" , extra_metrics = [ mlflow . metrics . genai . answer_similarity ()], evaluators = \"default\" , ) print ( f \"See per-data evaluation results below: \\n { results . tables [ 'eval_results_table' ] } \" )\n",
      "View Evaluation Results via the MLflow UI\n",
      "Your evaluation result is automatically logged into MLflow server, so you can view your evaluation results directly from the\n",
      "MLflow UI. To view the evaluation results on MLflow UI, please follow the steps below:\n",
      "Go to the experiment view of your MLflow experiment.\n",
      "Go to the experiment view of your MLflow experiment.\n",
      "Select the “Evaluation” tab.\n",
      "Select the “Evaluation” tab.\n",
      "Select the runs you want to check evaluation results.\n",
      "Select the runs you want to check evaluation results.\n",
      "Select the metrics from the dropdown menu on the right side.\n",
      "Select the metrics from the dropdown menu on the right side.\n",
      "Please see the screenshot below for clarity:\n"
     ]
    }
   ],
   "source": [
    "llms_evaluate_url = \"https://www.mlflow.org/docs/2.8.1/llms/llm-evaluate/index.html\"\n",
    "llms_evaluate_page_content = extract_text_from_url(llms_evaluate_url, \"mlflow-llm-evaluate\")\n",
    "\n",
    "print(llms_evaluate_page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-0.022500766441226006,\n",
       "  0.019651325419545174,\n",
       "  0.010113414376974106,\n",
       "  -0.009565983898937702,\n",
       "  0.003982902504503727,\n",
       "  -5.642079486278817e-05,\n",
       "  0.005119871813803911,\n",
       "  0.0005807668203487992,\n",
       "  -0.027483781799674034,\n",
       "  -0.03551275283098221,\n",
       "  0.013468176126480103,\n",
       "  0.005084780510514975,\n",
       "  -0.03851659595966339,\n",
       "  -0.002216037828475237,\n",
       "  -0.005948035046458244,\n",
       "  0.0022581478115171194,\n",
       "  0.009229104034602642,\n",
       "  -0.011446896940469742,\n",
       "  0.021672604605555534,\n",
       "  -0.012064510025084019,\n",
       "  -0.004607534036040306,\n",
       "  -0.019019674509763718,\n",
       "  -0.013826110400259495,\n",
       "  -0.03683219850063324,\n",
       "  -0.027722405269742012,\n",
       "  0.006972711067646742,\n",
       "  0.015791242942214012,\n",
       "  -0.03725329786539078,\n",
       "  -0.0038741184398531914,\n",
       "  0.008485161699354649,\n",
       "  0.004151342436671257,\n",
       "  0.0016449211398139596,\n",
       "  -0.025588832795619965,\n",
       "  -0.005326912738382816,\n",
       "  -0.010885430499911308,\n",
       "  0.0005022492841817439,\n",
       "  0.014991153962910175,\n",
       "  0.031189460307359695,\n",
       "  0.03618651255965233,\n",
       "  -0.0001607636222615838,\n",
       "  0.012948819436132908,\n",
       "  0.018837198615074158,\n",
       "  0.005614664405584335,\n",
       "  -0.008527271449565887,\n",
       "  0.0007750868680886924,\n",
       "  0.0002504666626919061,\n",
       "  0.01872490532696247,\n",
       "  -0.030627993866801262,\n",
       "  -0.014127898961305618,\n",
       "  0.013250607997179031,\n",
       "  0.014170008711516857,\n",
       "  0.006681450642645359,\n",
       "  -0.017826559022068977,\n",
       "  0.005533953662961721,\n",
       "  0.0066217947751283646,\n",
       "  0.0068253264762461185,\n",
       "  0.021981410682201385,\n",
       "  -0.0016457984456792474,\n",
       "  -0.008099153637886047,\n",
       "  0.016226379200816154,\n",
       "  -0.006088401656597853,\n",
       "  -0.0017598463455215096,\n",
       "  -0.030291114002466202,\n",
       "  0.0015870200004428625,\n",
       "  0.01906178519129753,\n",
       "  -0.02762414887547493,\n",
       "  -0.005158472806215286,\n",
       "  0.02832598052918911,\n",
       "  0.015384180471301079,\n",
       "  0.00015495157276745886,\n",
       "  -0.010962631553411484,\n",
       "  -0.0042074890807271,\n",
       "  0.017461605370044708,\n",
       "  -0.007014821283519268,\n",
       "  0.028859375044703484,\n",
       "  0.0001837925228755921,\n",
       "  -0.022949939593672752,\n",
       "  -0.029785793274641037,\n",
       "  -0.007383283693343401,\n",
       "  -0.003570575499907136,\n",
       "  0.014920970425009727,\n",
       "  0.0001062619048752822,\n",
       "  -0.008885206654667854,\n",
       "  0.026753874495625496,\n",
       "  0.01312427781522274,\n",
       "  0.004860193934291601,\n",
       "  0.015370143577456474,\n",
       "  -0.007183261215686798,\n",
       "  0.01084332074970007,\n",
       "  -0.018093256279826164,\n",
       "  0.004151342436671257,\n",
       "  -0.008281629532575607,\n",
       "  0.020535634830594063,\n",
       "  0.02407287247478962,\n",
       "  0.009088737890124321,\n",
       "  0.008302684873342514,\n",
       "  -0.004575951490551233,\n",
       "  0.0068253264762461185,\n",
       "  0.020353157073259354,\n",
       "  -0.022570950910449028,\n",
       "  0.0008294789004139602,\n",
       "  0.01480867713689804,\n",
       "  -0.034305598586797714,\n",
       "  -0.010457311756908894,\n",
       "  -0.03402486443519592,\n",
       "  -0.010127450339496136,\n",
       "  -0.004025012254714966,\n",
       "  -0.02113921009004116,\n",
       "  0.0017826559487730265,\n",
       "  -0.0017554599326103926,\n",
       "  -0.0159035362303257,\n",
       "  0.020690036937594414,\n",
       "  -0.01464023720473051,\n",
       "  -0.047921158373355865,\n",
       "  -0.010885430499911308,\n",
       "  0.0007198175298981369,\n",
       "  0.023932507261633873,\n",
       "  -0.00643931794911623,\n",
       "  -0.02933662012219429,\n",
       "  -0.012689141556620598,\n",
       "  0.02796102873980999,\n",
       "  0.006039273459464312,\n",
       "  0.02700653485953808,\n",
       "  -0.030459553003311157,\n",
       "  0.03744981065392494,\n",
       "  0.0023686864878982306,\n",
       "  0.012745288200676441,\n",
       "  -0.006397208198904991,\n",
       "  -0.0293646939098835,\n",
       "  -0.005386568605899811,\n",
       "  0.02313241735100746,\n",
       "  0.011292493902146816,\n",
       "  0.024606266990303993,\n",
       "  0.003895173314958811,\n",
       "  -0.029448913410305977,\n",
       "  0.005726957693696022,\n",
       "  -0.011088961735367775,\n",
       "  0.008253556676208973,\n",
       "  -0.0018317842623218894,\n",
       "  0.018767016008496284,\n",
       "  0.025069475173950195,\n",
       "  0.0333511047065258,\n",
       "  0.0019072312861680984,\n",
       "  -0.006074364762753248,\n",
       "  -0.028648825362324715,\n",
       "  0.03778669238090515,\n",
       "  0.021279577165842056,\n",
       "  -0.011755703017115593,\n",
       "  0.00742539344355464,\n",
       "  -0.01245051808655262,\n",
       "  0.008829060010612011,\n",
       "  -0.031133312731981277,\n",
       "  -0.02765222080051899,\n",
       "  0.009727406315505505,\n",
       "  0.0006219995557330549,\n",
       "  0.0008983463048934937,\n",
       "  0.009165939874947071,\n",
       "  0.04331713542342186,\n",
       "  -0.021855080500245094,\n",
       "  -0.021069027483463287,\n",
       "  0.012064510025084019,\n",
       "  0.019847838208079338,\n",
       "  0.008351813070476055,\n",
       "  0.0003072712861467153,\n",
       "  -0.026360848918557167,\n",
       "  0.023960579186677933,\n",
       "  0.04208190739154816,\n",
       "  -0.018345914781093597,\n",
       "  -0.02627662941813469,\n",
       "  -0.008611490949988365,\n",
       "  0.014162990264594555,\n",
       "  0.02930854819715023,\n",
       "  -0.03686027228832245,\n",
       "  0.021883154287934303,\n",
       "  -0.008878188207745552,\n",
       "  -0.01372083555907011,\n",
       "  0.019623251631855965,\n",
       "  -0.008836077526211739,\n",
       "  -0.0070744771510362625,\n",
       "  -0.014457760378718376,\n",
       "  0.02902781404554844,\n",
       "  0.00997304730117321,\n",
       "  -0.01321551576256752,\n",
       "  0.020325085148215294,\n",
       "  -0.009937955997884274,\n",
       "  -0.0397798977792263,\n",
       "  -0.006390189751982689,\n",
       "  -0.022893793880939484,\n",
       "  0.01404367946088314,\n",
       "  -0.010871393606066704,\n",
       "  0.005004069302231073,\n",
       "  0.055388662964105606,\n",
       "  0.002984544960781932,\n",
       "  -0.012611939571797848,\n",
       "  -0.5789842009544373,\n",
       "  -0.02401672676205635,\n",
       "  0.00913084764033556,\n",
       "  -0.0008768526604399085,\n",
       "  0.0014326167292892933,\n",
       "  -0.01688610203564167,\n",
       "  -0.024311495944857597,\n",
       "  -0.019777655601501465,\n",
       "  -0.008667637594044209,\n",
       "  0.04662978649139404,\n",
       "  -0.014233173802495003,\n",
       "  0.012499646283686161,\n",
       "  0.010225707665085793,\n",
       "  -0.01252772007137537,\n",
       "  -0.00019859681196976453,\n",
       "  -0.018037108704447746,\n",
       "  -0.021518200635910034,\n",
       "  -0.033603765070438385,\n",
       "  0.0043303100392222404,\n",
       "  -0.007628925144672394,\n",
       "  -0.03584963083267212,\n",
       "  0.025518648326396942,\n",
       "  -0.004495240747928619,\n",
       "  0.010450294241309166,\n",
       "  -0.04017292335629463,\n",
       "  -0.0077622733078897,\n",
       "  0.017180873081088066,\n",
       "  0.03949916362762451,\n",
       "  0.03074028715491295,\n",
       "  0.015173629857599735,\n",
       "  -0.023272782564163208,\n",
       "  0.0049233585596084595,\n",
       "  -0.018921418115496635,\n",
       "  -0.0018037109402939677,\n",
       "  0.04980207234621048,\n",
       "  -0.024943146854639053,\n",
       "  -0.012464554980397224,\n",
       "  0.013692762702703476,\n",
       "  -0.003803935134783387,\n",
       "  0.016900138929486275,\n",
       "  -0.01252772007137537,\n",
       "  0.004667189903557301,\n",
       "  0.007881584577262402,\n",
       "  0.013138314709067345,\n",
       "  -0.011797813698649406,\n",
       "  0.005235674325376749,\n",
       "  0.014892896637320518,\n",
       "  0.003681114176288247,\n",
       "  0.004825102165341377,\n",
       "  0.003223168198019266,\n",
       "  -0.033884499222040176,\n",
       "  0.022444620728492737,\n",
       "  0.003449509385973215,\n",
       "  0.00643931794911623,\n",
       "  0.0004754918918479234,\n",
       "  0.006456864066421986,\n",
       "  0.022767463698983192,\n",
       "  -0.0014326167292892933,\n",
       "  -0.0014106844319030643,\n",
       "  0.0006355975638143718,\n",
       "  -0.005853287409991026,\n",
       "  -0.01073804497718811,\n",
       "  -0.04707895964384079,\n",
       "  -0.016465002670884132,\n",
       "  -0.007313100155442953,\n",
       "  0.03368798643350601,\n",
       "  0.0027494309470057487,\n",
       "  -0.005730466917157173,\n",
       "  0.0013729608617722988,\n",
       "  -0.023483334109187126,\n",
       "  0.009004517458379269,\n",
       "  -0.008758876472711563,\n",
       "  0.001292250119149685,\n",
       "  -0.0006856031250208616,\n",
       "  0.030599920079112053,\n",
       "  0.004418038763105869,\n",
       "  0.031133312731981277,\n",
       "  0.0022177924402058125,\n",
       "  -0.0022546385880559683,\n",
       "  0.012703177519142628,\n",
       "  0.02520984224975109,\n",
       "  -0.014457760378718376,\n",
       "  0.010422220453619957,\n",
       "  -0.020535634830594063,\n",
       "  -0.00030836788937449455,\n",
       "  0.005874342285096645,\n",
       "  0.00884309597313404,\n",
       "  0.003930265083909035,\n",
       "  0.01973554491996765,\n",
       "  0.025925712659955025,\n",
       "  0.017265092581510544,\n",
       "  0.057831041514873505,\n",
       "  -0.014499870128929615,\n",
       "  -0.051318030804395676,\n",
       "  -0.004168888088315725,\n",
       "  0.006734088063240051,\n",
       "  -0.04185732081532478,\n",
       "  0.03102101944386959,\n",
       "  0.02147608995437622,\n",
       "  -0.03992026299238205,\n",
       "  -0.019777655601501465,\n",
       "  -0.00850621610879898,\n",
       "  0.029533134773373604,\n",
       "  0.01102579664438963,\n",
       "  0.0030933290254324675,\n",
       "  0.011755703017115593,\n",
       "  -0.004323291592299938,\n",
       "  -0.0034565276000648737,\n",
       "  0.004291709046810865,\n",
       "  -0.03809549659490585,\n",
       "  -0.028704971075057983,\n",
       "  -0.019174078479409218,\n",
       "  -0.01602986641228199,\n",
       "  -0.010927540250122547,\n",
       "  0.006579684559255838,\n",
       "  -0.030487626791000366,\n",
       "  0.009004517458379269,\n",
       "  -0.008695711381733418,\n",
       "  0.023076269775629044,\n",
       "  0.0023792139254510403,\n",
       "  -0.0006597230676561594,\n",
       "  -0.0009930937085300684,\n",
       "  0.029420841485261917,\n",
       "  0.00557606341317296,\n",
       "  0.0036670775152742863,\n",
       "  0.01911793276667595,\n",
       "  0.012513683177530766,\n",
       "  -0.011117035523056984,\n",
       "  -0.010625751689076424,\n",
       "  0.02664158120751381,\n",
       "  0.002184455282986164,\n",
       "  -0.007011312060058117,\n",
       "  0.023328930139541626,\n",
       "  -0.033210739493370056,\n",
       "  0.03231239318847656,\n",
       "  0.0060357642360031605,\n",
       "  0.022234071046113968,\n",
       "  -0.0021283086389303207,\n",
       "  0.015931610018014908,\n",
       "  -0.00040728249587118626,\n",
       "  -0.026529287919402122,\n",
       "  0.017391422763466835,\n",
       "  -0.005965580698102713,\n",
       "  -0.012899691238999367,\n",
       "  -0.035933852195739746,\n",
       "  -0.027146901935338974,\n",
       "  -0.023623699322342873,\n",
       "  0.014387576840817928,\n",
       "  0.010548550635576248,\n",
       "  0.006386680528521538,\n",
       "  -0.041464295238256454,\n",
       "  -0.020760221406817436,\n",
       "  -0.010759100317955017,\n",
       "  0.006737597286701202,\n",
       "  -0.02129361405968666,\n",
       "  -0.022079667076468468,\n",
       "  -0.007102550473064184,\n",
       "  -0.021883154287934303,\n",
       "  -0.013692762702703476,\n",
       "  -0.03267734497785568,\n",
       "  -0.017489679157733917,\n",
       "  0.019131967797875404,\n",
       "  -0.008709748275578022,\n",
       "  0.028241761028766632,\n",
       "  -0.004523314069956541,\n",
       "  0.0009606339735910296,\n",
       "  -0.023932507261633873,\n",
       "  0.02101288177073002,\n",
       "  -0.026936352252960205,\n",
       "  -0.03483899310231209,\n",
       "  -0.015833353623747826,\n",
       "  -0.017419496551156044,\n",
       "  -0.007092022802680731,\n",
       "  0.015370143577456474,\n",
       "  0.01704050600528717,\n",
       "  0.01451390702277422,\n",
       "  -0.010878412052989006,\n",
       "  -0.008218464441597462,\n",
       "  -0.0010202897246927023,\n",
       "  -0.021335724741220474,\n",
       "  0.007313100155442953,\n",
       "  0.0412958562374115,\n",
       "  -0.011166163720190525,\n",
       "  -0.008765894919633865,\n",
       "  0.03674797713756561,\n",
       "  0.019188115373253822,\n",
       "  0.023202599957585335,\n",
       "  0.013075149618089199,\n",
       "  -0.01820554956793785,\n",
       "  0.013068131171166897,\n",
       "  0.0016264980658888817,\n",
       "  0.021223431453108788,\n",
       "  -0.03542853146791458,\n",
       "  0.004081158898770809,\n",
       "  -0.00858341809362173,\n",
       "  0.017644083127379417,\n",
       "  -0.007263971958309412,\n",
       "  0.0027231122367084026,\n",
       "  0.004653153009712696,\n",
       "  0.03197551146149635,\n",
       "  -0.013440102338790894,\n",
       "  0.023637736216187477,\n",
       "  0.009580020792782307,\n",
       "  -0.01792481541633606,\n",
       "  -0.003695150837302208,\n",
       "  -0.018668759614229202,\n",
       "  0.02456415630877018,\n",
       "  -0.033323030918836594,\n",
       "  0.004491731524467468,\n",
       "  0.005733975674957037,\n",
       "  0.027301305904984474,\n",
       "  -0.01716683618724346,\n",
       "  -0.02746974490582943,\n",
       "  -0.023595627397298813,\n",
       "  -0.003521447302773595,\n",
       "  0.017672155052423477,\n",
       "  0.009446673095226288,\n",
       "  -0.006253332365304232,\n",
       "  -0.006530556362122297,\n",
       "  0.005147945135831833,\n",
       "  0.009186994284391403,\n",
       "  0.008646583184599876,\n",
       "  0.02734341472387314,\n",
       "  0.0051725092343986034,\n",
       "  0.02832598052918911,\n",
       "  -0.017026469111442566,\n",
       "  -0.028185615316033363,\n",
       "  -0.00028884815401397645,\n",
       "  0.003152984892949462,\n",
       "  -0.02520984224975109,\n",
       "  0.018921418115496635,\n",
       "  0.018654722720384598,\n",
       "  0.03919035568833351,\n",
       "  -0.006716541945934296,\n",
       "  0.03472669795155525,\n",
       "  0.01862664893269539,\n",
       "  0.03893769532442093,\n",
       "  -0.004874230362474918,\n",
       "  0.022037556394934654,\n",
       "  -0.027975065633654594,\n",
       "  0.0017256319988518953,\n",
       "  0.002996827010065317,\n",
       "  0.0021107627544552088,\n",
       "  -0.01880912482738495,\n",
       "  0.031526338309049606,\n",
       "  -0.012162766419351101,\n",
       "  0.025069475173950195,\n",
       "  0.011018778197467327,\n",
       "  -0.011713593266904354,\n",
       "  -0.008043006993830204,\n",
       "  -0.007242917083203793,\n",
       "  0.020044350996613503,\n",
       "  -0.013440102338790894,\n",
       "  -0.009643185883760452,\n",
       "  0.009495801292359829,\n",
       "  -0.025855528190732002,\n",
       "  -0.016198307275772095,\n",
       "  -0.006074364762753248,\n",
       "  0.01621234230697155,\n",
       "  0.006569157354533672,\n",
       "  -0.008372868411242962,\n",
       "  0.00576555822044611,\n",
       "  0.01271019596606493,\n",
       "  -0.013531341217458248,\n",
       "  0.0327334925532341,\n",
       "  -0.021911228075623512,\n",
       "  -0.00800089631229639,\n",
       "  -0.017391422763466835,\n",
       "  -0.02297801338136196,\n",
       "  -0.011306529864668846,\n",
       "  -0.027203047648072243,\n",
       "  -0.024241313338279724,\n",
       "  0.008183373138308525,\n",
       "  -0.024830853566527367,\n",
       "  0.04458043351769447,\n",
       "  0.022963976487517357,\n",
       "  0.01685803011059761,\n",
       "  0.029813867062330246,\n",
       "  0.007811401505023241,\n",
       "  0.007586814928799868,\n",
       "  -0.026753874495625496,\n",
       "  -0.0357934832572937,\n",
       "  0.043850526213645935,\n",
       "  -0.0070744771510362625,\n",
       "  -0.003719715168699622,\n",
       "  -0.00884309597313404,\n",
       "  -0.02661350928246975,\n",
       "  0.0014861314557492733,\n",
       "  -0.00034477547160349786,\n",
       "  0.020212791860103607,\n",
       "  0.019510958343744278,\n",
       "  -0.01358046941459179,\n",
       "  -0.00639369897544384,\n",
       "  0.004270654171705246,\n",
       "  -0.03208780661225319,\n",
       "  0.02741359919309616,\n",
       "  0.010162542574107647,\n",
       "  -0.007664016913622618,\n",
       "  0.011138089932501316,\n",
       "  0.00815530028194189,\n",
       "  0.0018019563285633922,\n",
       "  -0.009074700996279716,\n",
       "  -0.04196961596608162,\n",
       "  -0.029813867062330246,\n",
       "  0.018668759614229202,\n",
       "  -0.004737372975796461,\n",
       "  -0.010639788582921028,\n",
       "  -0.01838802546262741,\n",
       "  0.006723560392856598,\n",
       "  -0.0037267333827912807,\n",
       "  0.003610930871218443,\n",
       "  -0.0032389594707638025,\n",
       "  0.001096614170819521,\n",
       "  0.006769179832190275,\n",
       "  0.02287975698709488,\n",
       "  0.008007914759218693,\n",
       "  -0.009201031178236008,\n",
       "  0.007242917083203793,\n",
       "  0.0160579401999712,\n",
       "  0.026529287919402122,\n",
       "  -0.015580693259835243,\n",
       "  -0.03613036498427391,\n",
       "  -0.012927764095366001,\n",
       "  0.015468399971723557,\n",
       "  0.03899384289979935,\n",
       "  0.021419944241642952,\n",
       "  -0.0032161499839276075,\n",
       "  0.014352485537528992,\n",
       "  -0.03963952884078026,\n",
       "  -0.013110240921378136,\n",
       "  0.004010975826531649,\n",
       "  -0.00668495986610651,\n",
       "  0.013615560717880726,\n",
       "  0.025195805355906487,\n",
       "  -0.03161055967211723,\n",
       "  0.010752081871032715,\n",
       "  0.003014372894540429,\n",
       "  0.003793407464399934,\n",
       "  0.045871805399656296,\n",
       "  0.009179975837469101,\n",
       "  0.001758091733790934,\n",
       "  -0.014457760378718376,\n",
       "  0.005593609064817429,\n",
       "  0.02098480798304081,\n",
       "  0.0036705867387354374,\n",
       "  -0.015510509721934795,\n",
       "  0.011215291917324066,\n",
       "  0.0061199842020869255,\n",
       "  0.017054542899131775,\n",
       "  0.015707023441791534,\n",
       "  0.026052042841911316,\n",
       "  0.013264643959701061,\n",
       "  -0.011987308040261269,\n",
       "  -0.028311945497989655,\n",
       "  -0.010773137211799622,\n",
       "  -0.002370441099628806,\n",
       "  0.006155075505375862,\n",
       "  0.028859375044703484,\n",
       "  -0.027610111981630325,\n",
       "  0.06046993285417557,\n",
       "  0.03542853146791458,\n",
       "  -0.0062112221494317055,\n",
       "  0.009888827800750732,\n",
       "  0.01102579664438963,\n",
       "  0.01358046941459179,\n",
       "  0.0117767583578825,\n",
       "  0.00468824477866292,\n",
       "  0.007362228352576494,\n",
       "  -0.0035670665092766285,\n",
       "  -0.02233232744038105,\n",
       "  0.03287385776638985,\n",
       "  0.006312988232821226,\n",
       "  -0.005969089921563864,\n",
       "  -0.0018458209233358502,\n",
       "  0.009271214716136456,\n",
       "  -0.00798686034977436,\n",
       "  -0.044496215879917145,\n",
       "  -0.023609662428498268,\n",
       "  0.03312651813030243,\n",
       "  0.03380027785897255,\n",
       "  0.010197633877396584,\n",
       "  0.004463658202439547,\n",
       "  9.217041952069849e-05,\n",
       "  -0.01485078688710928,\n",
       "  -0.007207825314253569,\n",
       "  0.005228655878454447,\n",
       "  0.0033670440316200256,\n",
       "  0.00442154798656702,\n",
       "  -0.012127675116062164,\n",
       "  0.019847838208079338,\n",
       "  -0.0253923200070858,\n",
       "  0.011110017076134682,\n",
       "  -0.027048645541071892,\n",
       "  0.009362452663481236,\n",
       "  -0.009257177822291851,\n",
       "  -0.0004147394502069801,\n",
       "  -0.030263040214776993,\n",
       "  0.015215740539133549,\n",
       "  0.008232501335442066,\n",
       "  0.01182588655501604,\n",
       "  0.015777206048369408,\n",
       "  0.0001137188810389489,\n",
       "  -0.009734423831105232,\n",
       "  -0.004326800815761089,\n",
       "  0.007148169446736574,\n",
       "  -0.005298839416354895,\n",
       "  -0.011081943288445473,\n",
       "  -0.02025490067899227,\n",
       "  -0.007488558534532785,\n",
       "  -0.026023969054222107,\n",
       "  -0.006790234707295895,\n",
       "  0.009530892595648766,\n",
       "  -0.031189460307359695,\n",
       "  0.0020616345573216677,\n",
       "  -0.025476539507508278,\n",
       "  -0.0073692467994987965,\n",
       "  -0.010822265408933163,\n",
       "  0.0069165644235908985,\n",
       "  0.006341061554849148,\n",
       "  0.03879733011126518,\n",
       "  0.012057491578161716,\n",
       "  0.001580001669935882,\n",
       "  0.012815470807254314,\n",
       "  -0.032593127340078354,\n",
       "  0.005502371117472649,\n",
       "  -0.07776310294866562,\n",
       "  -0.018528392538428307,\n",
       "  0.0077412184327840805,\n",
       "  -0.010148505680263042,\n",
       "  0.007306081708520651,\n",
       "  -0.007678053341805935,\n",
       "  0.02964542806148529,\n",
       "  0.0006610389682464302,\n",
       "  0.00759383337572217,\n",
       "  0.0015466645127162337,\n",
       "  0.001158901839517057,\n",
       "  0.02217792347073555,\n",
       "  0.0014826222322881222,\n",
       "  0.01740545965731144,\n",
       "  -0.008281629532575607,\n",
       "  0.0169843602925539,\n",
       "  0.018795087933540344,\n",
       "  0.0016001793555915356,\n",
       "  -0.019960131496191025,\n",
       "  0.018893346190452576,\n",
       "  -0.04365401342511177,\n",
       "  0.04671400785446167,\n",
       "  0.020240863785147667,\n",
       "  -0.02391847036778927,\n",
       "  0.04031328856945038,\n",
       "  0.006305969785898924,\n",
       "  -0.01948288455605507,\n",
       "  -0.012176803313195705,\n",
       "  -0.0007132377941161394,\n",
       "  0.015735097229480743,\n",
       "  0.012576848268508911,\n",
       "  0.0018879309063777328,\n",
       "  -0.00828864797949791,\n",
       "  -0.03248083218932152,\n",
       "  -0.011664465069770813,\n",
       "  -0.04306447505950928,\n",
       "  0.008632546290755272,\n",
       "  -0.014155972748994827,\n",
       "  0.012436481192708015,\n",
       "  -0.008765894919633865,\n",
       "  0.01828976906836033,\n",
       "  0.006709523964673281,\n",
       "  -0.012211894616484642,\n",
       "  0.020760221406817436,\n",
       "  -0.029252400621771812,\n",
       "  -0.011018778197467327,\n",
       "  0.02181296981871128,\n",
       "  0.0071657150983810425,\n",
       "  -0.004554896615445614,\n",
       "  -0.033210739493370056,\n",
       "  0.023427186533808708,\n",
       "  -0.06748826801776886,\n",
       "  -0.03930265083909035,\n",
       "  -0.01434546709060669,\n",
       "  -0.009650204330682755,\n",
       "  0.011257401667535305,\n",
       "  0.021728750318288803,\n",
       "  0.0501670241355896,\n",
       "  0.005481315776705742,\n",
       "  0.049942437559366226,\n",
       "  -0.011839923448860645,\n",
       "  0.027020571753382683,\n",
       "  -0.002538881031796336,\n",
       "  -0.0013782245805487037,\n",
       "  -0.0060357642360031605,\n",
       "  0.016787845641374588,\n",
       "  -0.013362901285290718,\n",
       "  0.02178489789366722,\n",
       "  0.0011782022193074226,\n",
       "  -0.0019282862776890397,\n",
       "  0.007193788420408964,\n",
       "  0.013320790603756905,\n",
       "  0.010611715726554394,\n",
       "  -0.02272535301744938,\n",
       "  0.037674397230148315,\n",
       "  0.001709840726107359,\n",
       "  -0.0024985256604850292,\n",
       "  -0.030291114002466202,\n",
       "  -0.021588383242487907,\n",
       "  -0.009994102641940117,\n",
       "  0.024058835580945015,\n",
       "  -0.009046628139913082,\n",
       "  0.014963080175220966,\n",
       "  -0.04898794740438461,\n",
       "  -0.01624041609466076,\n",
       "  0.0321158803999424,\n",
       "  -0.006997275166213512,\n",
       "  0.011138089932501316,\n",
       "  0.027203047648072243,\n",
       "  -0.0012141711777076125,\n",
       "  -0.028719007968902588,\n",
       "  0.029083961620926857,\n",
       "  0.00836584996432066,\n",
       "  -0.002810841193422675,\n",
       "  -0.027848735451698303,\n",
       "  -0.017475642263889313,\n",
       "  -0.03340725228190422,\n",
       "  -0.013959459029138088,\n",
       "  0.0023423677776008844,\n",
       "  -0.012345243245363235,\n",
       "  0.027245158329606056,\n",
       "  -0.0001545129343867302,\n",
       "  -0.008836077526211739,\n",
       "  -0.00702183973044157,\n",
       "  0.0028354055248200893,\n",
       "  -0.013440102338790894,\n",
       "  -0.011496025137603283,\n",
       "  0.0066568865440785885,\n",
       "  -0.04163273423910141,\n",
       "  -0.006523537915199995,\n",
       "  -0.012780379503965378,\n",
       "  -0.0056673018261790276,\n",
       "  -0.006298951338976622,\n",
       "  0.001505431835539639,\n",
       "  0.010787174105644226,\n",
       "  0.03422137722373009,\n",
       "  0.015749134123325348,\n",
       "  -0.03966760262846947,\n",
       "  -0.02052159793674946,\n",
       "  0.022430583834648132,\n",
       "  0.01350326742976904,\n",
       "  0.04845455288887024,\n",
       "  -0.0052953301928937435,\n",
       "  0.02477470599114895,\n",
       "  0.012197857722640038,\n",
       "  0.0015431554056704044,\n",
       "  -0.018893346190452576,\n",
       "  -0.011306529864668846,\n",
       "  -0.004768955521285534,\n",
       "  -0.011846941895782948,\n",
       "  0.01930040866136551,\n",
       "  0.02716093882918358,\n",
       "  -0.017812522128224373,\n",
       "  -0.03405293822288513,\n",
       "  0.007635943591594696,\n",
       "  0.007028857711702585,\n",
       "  -0.016170233488082886,\n",
       "  -0.030768360942602158,\n",
       "  0.02928047440946102,\n",
       "  0.011601299978792667,\n",
       "  -0.01847224496304989,\n",
       "  -0.03037533350288868,\n",
       "  -0.005653264932334423,\n",
       "  -0.02116728387773037,\n",
       "  0.03475477173924446,\n",
       "  -0.0059620714746415615,\n",
       "  0.010015157051384449,\n",
       "  0.019960131496191025,\n",
       "  -0.0067411065101623535,\n",
       "  -0.0007246425957418978,\n",
       "  0.050279319286346436,\n",
       "  0.02425535023212433,\n",
       "  0.010808228515088558,\n",
       "  -0.007565760053694248,\n",
       "  -0.003803935134783387,\n",
       "  0.03318266570568085,\n",
       "  0.002923134481534362,\n",
       "  -0.04665786027908325,\n",
       "  0.032593127340078354,\n",
       "  0.009755479171872139,\n",
       "  0.036663759499788284,\n",
       "  -0.02254287712275982,\n",
       "  0.019384628161787987,\n",
       "  0.005965580698102713,\n",
       "  -0.026571398600935936,\n",
       "  -0.0008163195452652872,\n",
       "  0.0010527495760470629,\n",
       "  -0.011299511417746544,\n",
       "  0.0357934832572937,\n",
       "  -0.026655618101358414,\n",
       "  0.004881248809397221,\n",
       "  0.026164336130023003,\n",
       "  -0.030206894502043724,\n",
       "  -0.002238847315311432,\n",
       "  -0.014794640243053436,\n",
       "  -0.005863815080374479,\n",
       "  0.00022030978288967162,\n",
       "  0.009980065748095512,\n",
       "  -0.009580020792782307,\n",
       "  0.02643103152513504,\n",
       "  0.0008772913133725524,\n",
       "  -0.03276156634092331,\n",
       "  0.023427186533808708,\n",
       "  -0.004302236717194319,\n",
       "  -0.015005189925432205,\n",
       "  -0.0041267783381044865,\n",
       "  -0.028775153681635857,\n",
       "  0.0007847370579838753,\n",
       "  -0.036635685712099075,\n",
       "  -0.015159593895077705,\n",
       "  0.005411132704466581,\n",
       "  0.01875297911465168,\n",
       "  0.017054542899131775,\n",
       "  0.009004517458379269,\n",
       "  -0.002802068367600441,\n",
       "  0.01639482006430626,\n",
       "  0.03511972352862358,\n",
       "  -0.02471856027841568,\n",
       "  0.007341173477470875,\n",
       "  -0.014324412681162357,\n",
       "  0.003156494116410613,\n",
       "  -0.014892896637320518,\n",
       "  -0.015622803010046482,\n",
       "  -0.01136267650872469,\n",
       "  0.0023072760086506605,\n",
       "  0.007755254860967398,\n",
       "  -0.009502819739282131,\n",
       "  -0.044468142092227936,\n",
       "  0.0026985479053109884,\n",
       "  0.01408578921109438,\n",
       "  0.018359951674938202,\n",
       "  0.00020506684086285532,\n",
       "  -0.029785793274641037,\n",
       "  -0.006007690913975239,\n",
       "  0.010457311756908894,\n",
       "  0.009755479171872139,\n",
       "  -0.020240863785147667,\n",
       "  -0.004418038763105869,\n",
       "  0.014401613734662533,\n",
       "  -0.01813536509871483,\n",
       "  0.00473386375233531,\n",
       "  -0.017840595915913582,\n",
       "  -0.02067600190639496,\n",
       "  0.009930937550961971,\n",
       "  -0.008478143252432346,\n",
       "  -0.004088177345693111,\n",
       "  0.001749318791553378,\n",
       "  -0.03402486443519592,\n",
       "  -0.0018037109402939677,\n",
       "  -0.019833801314234734,\n",
       "  0.0074534667655825615,\n",
       "  -0.006467391271144152,\n",
       "  0.016773808747529984,\n",
       "  -0.020886551588773727,\n",
       "  -0.0038284992333501577,\n",
       "  -0.02746974490582943,\n",
       "  0.021433981135487556,\n",
       "  -0.028185615316033363,\n",
       "  7.19378876965493e-05,\n",
       "  -0.009250159375369549,\n",
       "  -0.0021019899286329746,\n",
       "  0.005084780510514975,\n",
       "  0.016956286504864693,\n",
       "  -0.03478284552693367,\n",
       "  -0.04031328856945038,\n",
       "  -0.014036661013960838,\n",
       "  0.020718110725283623,\n",
       "  -0.009523874148726463,\n",
       "  -0.03307037428021431,\n",
       "  0.009081719443202019,\n",
       "  0.06748826801776886,\n",
       "  0.008183373138308525,\n",
       "  -0.017630046233534813,\n",
       "  -0.03613036498427391,\n",
       "  -0.04738776758313179,\n",
       "  -0.03472669795155525,\n",
       "  -0.011650428175926208,\n",
       "  -0.01405771542340517,\n",
       "  0.01581931672990322,\n",
       "  -0.0192723348736763,\n",
       "  0.02630470134317875,\n",
       "  -0.008772913366556168,\n",
       "  0.01880912482738495,\n",
       "  -0.008934334851801395,\n",
       "  0.009123829193413258,\n",
       "  -0.01341202948242426,\n",
       "  0.00403904914855957,\n",
       "  0.006884981878101826,\n",
       "  -0.0029354167636483908,\n",
       "  0.011832905001938343,\n",
       "  -0.021209394559264183,\n",
       "  -0.025434428825974464,\n",
       "  0.0007540318765677512,\n",
       "  0.0038706092163920403,\n",
       "  -0.017461605370044708,\n",
       "  0.00542867835611105,\n",
       "  0.020114535465836525,\n",
       "  0.0007619275129400194,\n",
       "  -0.014892896637320518,\n",
       "  -0.0013238325482234359,\n",
       "  -0.008021951653063297,\n",
       "  0.0018949492368847132,\n",
       "  -0.004533841274678707,\n",
       "  0.009292269125580788,\n",
       "  -0.010625751689076424,\n",
       "  -0.01358046941459179,\n",
       "  -0.004958450328558683,\n",
       "  -0.0017308957176283002,\n",
       "  0.0011150372447445989,\n",
       "  -0.0025423902552574873,\n",
       "  0.028438273817300797,\n",
       "  -0.01072400901466608,\n",
       "  0.01235928013920784,\n",
       "  0.0045443689450621605,\n",
       "  -0.01955306902527809,\n",
       "  -0.004463658202439547,\n",
       "  -0.012689141556620598,\n",
       "  0.05075656622648239,\n",
       "  0.006288424134254456,\n",
       "  0.012983910739421844,\n",
       "  0.030150746926665306,\n",
       "  -0.006337552331387997,\n",
       "  0.013510285876691341,\n",
       "  -0.0015440327115356922,\n",
       "  0.019328482449054718,\n",
       "  0.0023862323723733425,\n",
       "  -0.0011922388803213835,\n",
       "  0.047219324856996536,\n",
       "  -0.006021727342158556,\n",
       "  -0.029196254909038544,\n",
       "  0.0026581925339996815,\n",
       "  0.009229104034602642,\n",
       "  -0.010345018468797207,\n",
       "  -0.01887930929660797,\n",
       "  -0.009418599307537079,\n",
       "  -0.028522495180368423,\n",
       "  -0.036354951560497284,\n",
       "  0.000816758198197931,\n",
       "  0.004933886229991913,\n",
       "  -0.006190167274326086,\n",
       "  0.002877515507861972,\n",
       "  -0.014991153962910175,\n",
       "  0.006435808725655079,\n",
       "  -0.0030477100517600775,\n",
       "  -0.0293646939098835,\n",
       "  0.011966253630816936,\n",
       "  -0.009706350974738598,\n",
       "  -0.017419496551156044,\n",
       "  -0.012197857722640038,\n",
       "  0.0009264196269214153,\n",
       "  0.001736159436404705,\n",
       "  0.015510509721934795,\n",
       "  0.027904881164431572,\n",
       "  -0.02168664149940014,\n",
       "  0.012169784866273403,\n",
       "  0.19797305762767792,\n",
       "  -0.016450965777039528,\n",
       "  -0.01042923890054226,\n",
       "  0.019230226054787636,\n",
       "  0.008379886858165264,\n",
       "  0.00353372935205698,\n",
       "  0.023609662428498268,\n",
       "  0.029505060985684395,\n",
       "  -0.001532627851702273,\n",
       "  -0.003428454278036952,\n",
       "  0.01501922681927681,\n",
       "  -0.007523650303483009,\n",
       "  -0.005639228504151106,\n",
       "  0.009074700996279716,\n",
       "  0.02168664149940014,\n",
       "  -0.020577743649482727,\n",
       "  -0.030515700578689575,\n",
       "  -0.038376230746507645,\n",
       "  -0.012141711078584194,\n",
       "  0.007481540087610483,\n",
       "  -0.009818644262850285,\n",
       "  0.0031231569591909647,\n",
       "  -0.014162990264594555,\n",
       "  -0.03301422670483589,\n",
       "  0.017026469111442566,\n",
       "  -0.016198307275772095,\n",
       "  0.01807921938598156,\n",
       "  0.011860977858304977,\n",
       "  0.010043230839073658,\n",
       "  0.005986635573208332,\n",
       "  -0.012001344934105873,\n",
       "  0.01844417303800583,\n",
       "  0.004772464744746685,\n",
       "  -0.00033008086029440165,\n",
       "  -0.004709299653768539,\n",
       "  -0.0032617689575999975,\n",
       "  0.03613036498427391,\n",
       "  -0.0007606115541420877,\n",
       "  0.028733044862747192,\n",
       "  0.0025283535942435265,\n",
       "  0.024648375809192657,\n",
       "  0.0125417560338974,\n",
       "  0.0030494644306600094,\n",
       "  -0.03377220407128334,\n",
       "  0.008239519782364368,\n",
       "  -0.003116138745099306,\n",
       "  ...]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_evaluate_page = model.predict(llms_evaluate_page_content)\n",
    "\n",
    "embeddings_evaluate_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(embedding1, embedding2):\n",
    "    \"\"\"\n",
    "    Calculate the cosine similarity between two document embeddings.\n",
    "\n",
    "    :param embedding1: First embedding vector, either a list or a numpy array.\n",
    "    :param embedding2: Second embedding vector, either a list or a numpy array.\n",
    "    :return: Cosine similarity between the two embeddings.\n",
    "    \"\"\"\n",
    "    # Convert embeddings to numpy arrays if they are lists\n",
    "    if isinstance(embedding1, list):\n",
    "        embedding1 = np.array(embedding1)\n",
    "    if isinstance(embedding2, list):\n",
    "        embedding2 = np.array(embedding2)\n",
    "\n",
    "    # Flatten the embeddings to 1D arrays if they are 2D\n",
    "    embedding1 = embedding1.flatten()\n",
    "    embedding2 = embedding2.flatten()\n",
    "\n",
    "    return np.dot(embedding1, embedding2) / (np.linalg.norm(embedding1) * np.linalg.norm(embedding2))\n",
    "\n",
    "def euclidean_distance(embedding1, embedding2):\n",
    "    \"\"\"\n",
    "    Calculate the Euclidean distance between two document embeddings.\n",
    "\n",
    "    :param embedding1: First embedding vector, either a list or a numpy array.\n",
    "    :param embedding2: Second embedding vector, either a list or a numpy array.\n",
    "    :return: Euclidean distance between the two embeddings.\n",
    "    \"\"\"\n",
    "    # Convert embeddings to numpy arrays if they are lists\n",
    "    if isinstance(embedding1, list):\n",
    "        embedding1 = np.array(embedding1)\n",
    "    if isinstance(embedding2, list):\n",
    "        embedding2 = np.array(embedding2)\n",
    "\n",
    "    # Compute Euclidean distance\n",
    "    return np.linalg.norm(embedding1 - embedding2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8792430026458967\n",
      "0.4914407261221534\n"
     ]
    }
   ],
   "source": [
    "similarity_cos = cosine_similarity(llms_landing_embeddings, embeddings_evaluate_page)\n",
    "similarity_euclid = euclidean_distance(llms_landing_embeddings, embeddings_evaluate_page)\n",
    "print(similarity_cos)\n",
    "print(similarity_euclid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
