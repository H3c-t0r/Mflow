import csv
import logging
import mlflow
import os
import tempfile

from ast import literal_eval
from mlflow.tracking.client import MlflowClient
from mlflow.utils.annotations import experimental
from typing import List

_logger = logging.getLogger(__name__)


@experimental
def log_predictions(
    inputs: List[str],
    outputs: List[str],
    targets: List[str] = None,
    intermediate_outputs: List[str] = None,
    output_feedbacks: List[str] = None,
    artifact_path: str = "llm_predictions.csv",
) -> None:
    """
    Log a batch of inputs, outputs, targets outputs, intermediate_outputs and output_feedbacks
    for the current evaluation run. If no run is active, this method will create a new active run.

    :param inputs: List of input strings for which we want to log predictions
    :param outputs: List of output strings generated by the model
    :param targets: List of target strings as the expected outputs for the corresponding inputs
    :param intermediate_outputs: List of intermediate_outputs strings generated by the model
    :param output_feedbacks: List of output feedbacks generated during model evaluation
    :param artifact_path: The run-relative artifact path to which to log the predictions.
    :returns: None

    .. test-code-block:: python
        :caption: Example

        import mlflow

        inputs = [
            "{'question': 'How do I create a Databricks cluster with UC enabled?',"
            "'context': 'Databricks clusters are amazing'}",
        ]

        outputs = [
            "<Instructions for cluster creation with UC enabled>",
        ]

        targets = [
            "<Expected Instructions for cluster creation with UC enabled>"
        ]

        intermediate_outputs = [
            "Context: DataBricks clusters are ... Question:"
            " How do I create a Databricks cluster with UC access? Answer:",
        ]

        output_feedbacks = [
            "{feedback: 'thumps up', score: 0.9, toxicity: 0.1}",
        ]

        artifact_path = "llm_predictions.csv"


        with mlflow.start_run():
            # Log llm predictions
            mlflow.llm.log_predictions(
                inputs,
                outputs,
                targets,
                intermediate_outputs,
                output_feedbacks,
                artifact_path
            )
    """
    if len(inputs) <= 0 or len(inputs) != len(outputs):
        raise ValueError("The length of inputs, outputs must be the same and not empty.")

    run_artifact_path = None
    predictions = []
    run = mlflow.tracking.fluent._get_or_start_run()
    run_id = run.info.run_id
    client = MlflowClient()
    LLM_ARTIFACT_NAME = artifact_path

    local_variables = vars()
    schema = list(
        filter(
            lambda x: local_variables[x] is not None,
            ["inputs", "outputs", "targets", "intermediate_outputs", "output_feedbacks"],
        )
    )

    for row in zip(*[local_variables[x] for x in schema]):
        predictions.append(row)

    with tempfile.TemporaryDirectory() as tmpdir:
        artifacts = [f.path for f in client.list_artifacts(run_id)]
        if LLM_ARTIFACT_NAME in artifacts:
            run_artifact_path = mlflow.artifacts.download_artifacts(
                run_id=run_id, artifact_path=LLM_ARTIFACT_NAME, dst_path=tmpdir
            )
            _logger.info(
                "Appending new inputs to already existing artifact "
                f"{LLM_ARTIFACT_NAME} for run {run_id}."
            )
        else:
            # If the artifact doesn't exist, we need to write the header.
            predictions.insert(0, schema)
            run_artifact_path = os.path.join(tmpdir, LLM_ARTIFACT_NAME)
            _logger.info(f"Creating a new {LLM_ARTIFACT_NAME} for run {run_id}.")

        if os.path.exists(run_artifact_path):
            with open(run_artifact_path, newline="") as llm_prediction:
                num_existing_predictions = sum(1 for _ in csv.reader(llm_prediction))
                if num_existing_predictions + len(predictions) > 1000:
                    _logger.warning(
                        f"Trying to log a {LLM_ARTIFACT_NAME} with length "
                        "more than 1000 records. It might slow down performance."
                    )

        with open(run_artifact_path, "a", encoding="UTF8", newline="") as llm_prediction:
            writer = csv.writer(llm_prediction)
            writer.writerows(predictions)

        TAG_NAME = "llm_predictions"
        # Get the current value of the tag
        current_tag_value = literal_eval(run.data.tags.get(TAG_NAME, "[]"))

        # Append the new + deduplicate the new value to the list
        current_tag_value.append(LLM_ARTIFACT_NAME)
        new_tag_value = list(set(current_tag_value))

        # Set the tag with the updated list
        client.set_tag(run_id, TAG_NAME, new_tag_value)

        mlflow.tracking.fluent.log_artifact(run_artifact_path)
