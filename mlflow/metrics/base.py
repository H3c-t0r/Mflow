from dataclasses import dataclass
from typing import Dict, List, Union, Optional

from mlflow.exceptions import MlflowException
from mlflow.utils.annotations import experimental


@experimental
@dataclass
class MetricValue:
    """
    The value of a metric.

    :param scores: The value of the metric per row
    :param justifications: The justification (if applicable) for the respective score
    :param aggregate_results: A dictionary mapping the name of the aggregation to its value
    """

    scores: List[float] = None
    justifications: List[str] = None
    aggregate_results: Dict[str, float] = None


@experimental
@dataclass
class EvaluationExample:
    """
    Stores the sample example during few shot learning during LLM evaluation

    :param input: The input provided to the model
    :param output: The output generated by the model
    :param score: The score given by the evaluator
    :param justification: The justification given by the evaluator
    :param grading_context: The grading_context provided to the evaluator for evaluation

    .. code-block:: python
        :caption: Example for creating an EvaluationExample

        from mlflow.metrics.base import EvaluationExample

        example = EvaluationExample(
            input="What is MLflow?",
            output="MLflow is an open-source platform for managing machine "
            "learning workflows, including experiment tracking, model packaging, "
            "versioning, and deployment, simplifying the ML lifecycle.",
            score=4,
            justification="The definition effectively explains what MLflow is "
            "its purpose, and its developer. It could be more concise for a 5-score.",
            grading_context={
                "ground_truth": "MLflow is an open-source platform for managing "
                "the end-to-end machine learning (ML) lifecycle. It was developed by Databricks, "
                "a company that specializes in big data and machine learning solutions. MLflow is "
                "designed to address the challenges that data scientists and machine learning "
                "engineers face when developing, training, and deploying machine learning models."
            },
        )
        print(str(example))

    .. code-block:: text
        :caption: Output

        Input: What is MLflow?
        Provided output: "MLflow is an open-source platform for managing machine "
            "learning workflows, including experiment tracking, model packaging, "
            "versioning, and deployment, simplifying the ML lifecycle."
        Provided ground_truth: "MLflow is an open-source platform for managing "
            "the end-to-end machine learning (ML) lifecycle. It was developed by Databricks, "
            "a company that specializes in big data and machine learning solutions. MLflow is "
            "designed to address the challenges that data scientists and machine learning "
            "engineers face when developing, training, and deploying machine learning models."
        Score: 4
        Justification: "The definition effectively explains what MLflow is "
            "its purpose, and its developer. It could be more concise for a 5-score."
    """

    input: str
    output: str
    score: float
    justification: str
    grading_context: Optional[Union[str, Dict[str, str]]] = None

    @staticmethod
    def _format_grading_context(grading_context):
        return "\n".join([f"key: {key}\nvalue:\n{value}" for key, value in grading_context.items()])

    def __str__(self) -> str:


    def to_prompt_component(self, grading_context_columns: List[str]) -> str:
        if self.grading_context is None and len(grading_context_columns) == 0:
            grading_context = {}
        elif isinstance(self.grading_context, dict):
            grading_context = self.grading_context
        else:
            grading_context = {grading_context_columns[0]: self.grading_context}

        if set(grading_context.keys()) != set(grading_context_columns):
            raise MlflowException.invalid_parameter_value(
                f"Example grading context does not contain required columns.\n"
                f" Example grading context columns: {list(grading_context.keys())}\n"
                f" Required grading context columns: {grading_context_columns}\n"
            )

        grading_context = (
            (
                "Additional information used by the model:\n"
                f"{EvaluationExample._format_grading_context(grading_context)}"
            )
            if self.grading_context
            else ""
        )

        return f"""
Input:
{self.input}

Output:
{self.output}

{grading_context}

score: {self.score}
justification: {self.justification}
        """
